{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nazanin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import sklearn\n",
    "import scipy as sp\n",
    "import nltk\n",
    "import re \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.display.max_colwidth = 200\n",
    "%matplotlib inline\n",
    "#import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "#from matplotlib import pyplot\n",
    "import pylab\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from scipy.stats import norm \n",
    "from scipy import stats \n",
    "from statsmodels.stats.weightstats import ztest\n",
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Nazanin\\Downloads\\drugsCom_raw\\drugsComTest_raw.csv', sep=\"\\t\")\n",
    "df1 = pd.read_csv(r'C:\\Users\\Nazanin\\Downloads\\drugsCom_raw\\drugsComTrain_raw.csv', sep=\"\\t\")\n",
    "#display(df, df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'drugName',\n",
       " 'condition',\n",
       " 'review',\n",
       " 'rating',\n",
       " 'date',\n",
       " 'usefulCount']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.append(df1)\n",
    "list(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df2 = df2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0         drugName                     condition  \\\n",
      "0      163740      Mirtazapine                    Depression   \n",
      "1      206473       Mesalamine  Crohn's Disease, Maintenance   \n",
      "2      159672          Bactrim       Urinary Tract Infection   \n",
      "3       39293         Contrave                   Weight Loss   \n",
      "4       97768  Cyclafem 1 / 35                 Birth Control   \n",
      "\n",
      "                                                                                                                                                                                                    review  \\\n",
      "0  \"I&#039;ve tried a few antidepressants over the years (citalopram, fluoxetine, amitriptyline), but none of those helped with my depression, insomnia &amp; anxiety. My doctor suggested and changed ...   \n",
      "1  \"My son has Crohn&#039;s disease and has done very well on the Asacol.  He has no complaints and shows no side effects.  He has taken as many as nine tablets per day at one time.  I&#039;ve been v...   \n",
      "2                                                                                                                                                                            \"Quick reduction of symptoms\"   \n",
      "3  \"Contrave combines drugs that were used for alcohol, smoking, and opioid cessation. People lose weight on it because it also helps control over-eating. I have no doubt that most obesity is caused ...   \n",
      "4  \"I have been on this birth control for one cycle. After reading some of the reviews on this type and similar birth controls I was a bit apprehensive to start. Im giving this birth control a 9 out ...   \n",
      "\n",
      "   rating                date  usefulCount  \n",
      "0    10.0   February 28, 2012           22  \n",
      "1     8.0        May 17, 2009           17  \n",
      "2     9.0  September 29, 2017            3  \n",
      "3     9.0       March 5, 2017           35  \n",
      "4     9.0    October 22, 2015            4  \n"
     ]
    }
   ],
   "source": [
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Levonorgestrel                        4930\n",
       "Etonogestrel                          4421\n",
       "Ethinyl estradiol / norethindrone     3753\n",
       "Nexplanon                             2892\n",
       "Ethinyl estradiol / norgestimate      2790\n",
       "Ethinyl estradiol / levonorgestrel    2503\n",
       "Phentermine                           2085\n",
       "Sertraline                            1868\n",
       "Escitalopram                          1747\n",
       "Mirena                                1673\n",
       "Name: drugName, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.drugName.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Birth Control      38436\n",
       "Depression         12164\n",
       "Pain                8245\n",
       "Anxiety             7812\n",
       "Acne                7435\n",
       "Bipolar Disorde     5604\n",
       "Insomnia            4904\n",
       "Weight Loss         4857\n",
       "Obesity             4757\n",
       "ADHD                4509\n",
       "Name: condition, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.condition.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4     \"I have been on this birth control for one cycle. After reading some of the reviews on this type and similar birth controls I was a bit apprehensive to start. Im giving this birth control a 9 out ...\n",
      "6     \"I&#039;ve had the copper coil for about 3 months now. I was really excited at the thought of not taking hormones. I&#039;m good with pain however I nearly fainted with insertion, couldn&#039;t be...\n",
      "9     \"I was on this pill for almost two years. It does work as far as not getting pregnant however my experience at first was it didn&#039;t make a huge difference then 6 or 7 months into it my sex dri...\n",
      "30    \"I absolutely love this product and recommend to everyone. I know everyone&#039;s body is different, so it is not for everyone, but it is not the medicines fault. I have NO negative symptoms since...\n",
      "37    \"I was on this for 5 years (and birth control pills for about 12 years), and would have told you how fabulous it was.  &lt;List all the benefits everyone else has listed, here.&gt;  Then a friend ...\n",
      "Name: review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df2[df2['condition'] == 'Birth Control'].review.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48                                                                                                                                                                                              \"Works for me\"\n",
      "63     \"I was prescribed this for onset of anxiety and possible hormonal mood swings. I was not told by my doctor how it would make me feel or how hard coming off of it would be. I took one 37.5 mg capsu...\n",
      "83     \"I did not like this medication. For anxiety, I have also tried Hydroxyzine (Atarax). I guess this is just my personal body chemistry but I actually prefer Atarax to this unlike most people. This ...\n",
      "133    \"I&#039;m a 32 year old male and I&#039;ve been taking buspar for about 10 months. At first it did nothing but make my anxiety worse. I would wake up with to full blown panic attacks and have the ...\n",
      "208    \"Klonopin is a very effective medicine for people such as myself that suffer from debilitating panic disorder and/or PTSD.  This medicine saved me from becoming institutionalized after returning h...\n",
      "Name: review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df2[df2['condition'] == 'Anxiety'].review.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP \n",
    "+ On the Machine Learning phase, we start looking at the text documents of the review column. In order to do that, we need to do some text pre-processing. We process and tokenize corpus of reviews to build features for predictive models. Also, train machine learning models to predict drug rating based on reviews and relevant metadata.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text pre-processing\n",
    "\n",
    "+ Removing tags: Removing unnecessary HTML tags, etc. which do not add much value when analyzing text (The BeautifulSoup library does an excellent job in providing necessary functions for this.).\n",
    "\n",
    "+ Removing accented characters: Removing  accented characters\\letters in the text corpus and convert these characters and standardized into ASCII characters, an example is to convert é to e. \n",
    "\n",
    "+ Expanding contractions: Converting contraction to its expanded, original form often helps with text standardization, example of which would be, \"do not\" to \"don’t\" and \"I would\" to \"I’d\".\n",
    "\n",
    "+ Removing special characters: Removing special characters and symbols often add to the extra noise in unstructured text. More than often, simple regular expressions (regexes) can be used to achieve this.\n",
    "\n",
    "+ Stemming and lemmatization: The reverse process of inflection is called stemming, which is basically obtaining the base form of a word. Lemmatization is very similar to stemming, the difference being that the root word from lemmatization is always a lexicographically correct word but the root stem may not be so.\n",
    "\n",
    "+ Removing stopwords: Words with little or no significance in text corpus are known as stopwords, example of which are:  a, an, the. We can use a standard English language stopwords list from nltk and also add our own domain specific stopwords as needed.\n",
    "\n",
    "+ Other text cleaning to do would be tokenization, removing extra whitespaces, text lower casing as well as spelling corrections, grammatical error corrections, removing repeated characters to name a few. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove special characters, extra whitespaces, digits, stopwords and lower casing the text corpus\n",
    "#result = re.sub(pattern, repl, string, count=0, flags=0);\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)   ###plscheck this \n",
    "wpt = nltk.WordPunctTokenizer()  # $4.99 \n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words.append('</span> users found this comment helpful.') \n",
    "stop_words.append('&#039')\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A) \n",
    "    #re.I A means treats . as whatever it is. Performs case-insensitive matching. #issues with FLAGS!!!!!\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words] \n",
    "    filtered_tokens = [stemmer.stem(word) for word in filtered_tokens]  \n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "# doc is the list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_corpus = normalize_corpus(df2[df2['condition'] == 'Birth Control'].review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['birth control one cycl read review type similar birth control bit apprehens start im give birth control long enough far love birth control side effect minim like im even birth control experienc mild headach nausea ive feel great got period cue third day inact pill idea come zero pms period light bare cramp unprotect sex first month obvious didnt get pregnant im pleas high recommend',\n",
       "       'ive copper coil month realli excit thought take hormon im good pain howev near faint insert couldnt beliv pain doctor say pain well month period last day im pain day random twang especi left side im consid whether want put intens pain heavi period id recommend somebodi doesnt alreadi heavi pain period right isnt',\n",
       "       'pill almost two year work far get pregnant howev experi first didnt make huge differ month sex drive went along dri moodi increas drastic would cri one second get angri husband anyth everyth skin gotten lot wors broke place never last week yaz',\n",
       "       ...,\n",
       "       'experi pain insert expect sinc ive never children lot bloat cramp breast tender first coupl week use pamprin heat pad belli help week insert came brown sludg last ten day first month studi miseri period stop breez occasion light cramp spot drawback person year old perimenopaus period make imposs track cycl overal delight',\n",
       "       'nexplanon sinc dec got first period end januari last month half march didnt bleed close three week start bleed march th bleed everi sinc gain lbs far sinc get birth control although weight gain isnt deal breaker bleed tri patient see bodi adjust implant three month far finger cross cycl go away awhil',\n",
       "       'would second month junel ive birth control year chang due spot increas mood swing previous birth control sinc switch shorter period day gain major weight increas appetit switch regular exercis routin still manag drop extra lbs'],\n",
       "      dtype='<U2960')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=None):\n",
    "    \"\"\"\n",
    "    List the top n words in a vocabulary according to occurrence in a text corpus.\n",
    "    \n",
    "    get_top_n_words([\"I love Python\", \"Python is a language programming\", \"Hello world\", \"I love the world\"]) -> \n",
    "    [('python', 2),\n",
    "     ('world', 2),\n",
    "     ('love', 2),\n",
    "     ('hello', 1),\n",
    "     ('is', 1),\n",
    "     ('programming', 1),\n",
    "     ('the', 1),\n",
    "     ('language', 1)]\n",
    "    \"\"\"\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('period', 37273),\n",
       " ('month', 36515),\n",
       " ('pill', 26390),\n",
       " ('get', 23071),\n",
       " ('ive', 21923),\n",
       " ('day', 21659),\n",
       " ('im', 20430),\n",
       " ('year', 19498),\n",
       " ('control', 18934),\n",
       " ('week', 18639),\n",
       " ('birth', 18338),\n",
       " ('take', 18146),\n",
       " ('first', 17184),\n",
       " ('cramp', 16278),\n",
       " ('gain', 15381),\n",
       " ('weight', 15367),\n",
       " ('start', 15324),\n",
       " ('effect', 14001),\n",
       " ('got', 13875),\n",
       " ('time', 13647),\n",
       " ('pain', 13167),\n",
       " ('like', 13144),\n",
       " ('side', 12075),\n",
       " ('bleed', 11932),\n",
       " ('would', 11297),\n",
       " ('insert', 11113),\n",
       " ('feel', 10858),\n",
       " ('sinc', 10519),\n",
       " ('mood', 10392),\n",
       " ('acn', 10089),\n",
       " ('never', 9849),\n",
       " ('spot', 9770),\n",
       " ('sex', 9433),\n",
       " ('bad', 9376),\n",
       " ('go', 9145),\n",
       " ('use', 9100),\n",
       " ('also', 9021),\n",
       " ('one', 8951),\n",
       " ('last', 8476),\n",
       " ('dont', 8321)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n_words(normalize_corpus(df2[df2['condition'] == 'Birth Control'].review), n=40)  #None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed text vector is \n",
      " [[5 0 1 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 1]\n",
      " [2 0 0 ... 0 0 0]]\n",
      "Words for each feature:\n",
      "['birth control', 'feel like', 'first month', 'gain weight', 'mood swing', 'sex drive', 'side effect', 'take pill', 'weight gain']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(2,2), min_df=0.05, max_df=1.0, max_features=500)\n",
    "\n",
    "# call `fit` to build the vocabulary\n",
    "vectorizer.fit(norm_corpus)\n",
    "\n",
    "# call `transform` to convert text to a bag of words\n",
    "x = vectorizer.transform(norm_corpus)\n",
    "\n",
    "# CountVectorizer uses a sparse array to save memory, but it's easier in this assignment to \n",
    "# convert back to a \"normal\" numpy array\n",
    "x = x.toarray()  # this is for visualization purposes \n",
    "\n",
    "print()\n",
    "print(\"Transformed text vector is \\n\", x)\n",
    "\n",
    "# `get_feature_names` tracks which word is associated with each column of the transformed x\n",
    "print\n",
    "print(\"Words for each feature:\")\n",
    "print(vectorizer.get_feature_names())  # visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38436, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df2[df2['condition'] == 'Birth Control'].rating  # df2['rating']\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y)  #(default=0.25)\n",
    "clf = MultinomialNB().fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = clf.score(xtrain, ytrain)\n",
    "test_accuracy = clf.score(xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MN Accuracy: 20.79%\n",
      "Accuracy on training data: 0.21\n",
      "Accuracy on test data:     0.21\n"
     ]
    }
   ],
   "source": [
    "print(\"MN Accuracy: %0.2f%%\" % (100 * clf.score(xtest, ytest)))\n",
    "print(\"Accuracy on training data: %0.2f\" % (training_accuracy))\n",
    "print(\"Accuracy on test data:     %0.2f\" % (test_accuracy))   #30% is precicely predicting the ratings for df2_min = 0.1\n",
    "# do a good and bad as a binary! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nazanin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    " #regressor target variables rating is \n",
    "# Train the model with Random Foprest Classifier     \n",
    "clf_RF = RandomForestClassifier()  \n",
    "#n_jobs=2 number of cores the computer uses, n_estimators = 500 (number of trees)  startt 50 \n",
    "clf_RF.fit(xtrain, ytrain)  \n",
    "# predict and evaluate performance\n",
    "clf_RF_predictions = clf_RF.predict(xtest)\n",
    "#meu.display_model_performance_metrics(true_labels=ytest, predicted_labels=clf_RF_predictions , classes= range(1,11))\n",
    "#hash maps a value to another value and it's more efficient in computation\n",
    "# normaization removing bias that could exists in a data set \n",
    "# correcting for the size of the corpus. one word review and the other is 200000 words \n",
    "# and contains the word good and we should \n",
    "# normaliztion in NLP is to use word frequecy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20584546 0.06917504 0.0826262  0.08909746 0.15301874 0.08619107\n",
      " 0.10124585 0.12122744 0.09157273]\n"
     ]
    }
   ],
   "source": [
    "print(clf_RF.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "print(clf_RF.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': None, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "#gridsearch and hyoerparameter tuning (on the TRAIN set) \n",
    "param_grid = {'n_estimators': [100, 200, 300, 500], 'max_features': ['auto', None, 'log2']}\n",
    "clf_RF = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "clf_RF.fit(xtrain, ytrain)\n",
    "print(clf_RF.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 'auto', 'n_estimators': 100} 0.2268 0.0033\n",
      "{'max_features': 'auto', 'n_estimators': 200} 0.2258 0.003\n",
      "{'max_features': 'auto', 'n_estimators': 300} 0.2267 0.0026\n",
      "{'max_features': 'auto', 'n_estimators': 500} 0.2268 0.003\n",
      "{'max_features': None, 'n_estimators': 100} 0.2269 0.0036\n",
      "{'max_features': None, 'n_estimators': 200} 0.2259 0.0028\n",
      "{'max_features': None, 'n_estimators': 300} 0.2266 0.0027\n",
      "{'max_features': None, 'n_estimators': 500} 0.2268 0.0028\n",
      "{'max_features': 'log2', 'n_estimators': 100} 0.2268 0.0033\n",
      "{'max_features': 'log2', 'n_estimators': 200} 0.2258 0.003\n",
      "{'max_features': 'log2', 'n_estimators': 300} 0.2267 0.0026\n",
      "{'max_features': 'log2', 'n_estimators': 500} 0.2268 0.003\n"
     ]
    }
   ],
   "source": [
    "results_22gram = clf_RF.cv_results_\n",
    "for param, score_mean, score_sd in zip(results['params'], results['mean_test_score'], results['std_test_score']):\n",
    "    print(param, round(score_mean, 4), round(score_sd, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best hyperparameter performance occurs at: {'max_features': None, 'n_estimators': 100} 0.2269 0.0036"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RF = RandomForestClassifier(n_estimators=100, max_features=None, random_state=42)\n",
    "clf_RF.fit(xtrain, ytrain)\n",
    "clf_RF_predictions = clf_RF.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 296   20   16    7    9    6   12   36   70  990]\n",
      " [ 145   23    7    6    8    4   11   30   45  464]\n",
      " [ 108   21   38    3    6    3    6   27   44  426]\n",
      " [ 100   11    3   14   11    6   12   29   30  297]\n",
      " [ 128   17    5    7   26    4    4   38   55  468]\n",
      " [  73   17    4    4    4   27    7   22   43  280]\n",
      " [  79    7    6    6    9    2   24   33   61  385]\n",
      " [ 144   17    6    7   14    6   17  123   63  651]\n",
      " [ 159   14   20    3   10    6   15   66  194  937]\n",
      " [ 169   29   10    3   10    7   17   75  125 1447]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.21      0.20      0.21      1462\n",
      "         2.0       0.13      0.03      0.05       743\n",
      "         3.0       0.33      0.06      0.10       682\n",
      "         4.0       0.23      0.03      0.05       513\n",
      "         5.0       0.24      0.03      0.06       752\n",
      "         6.0       0.38      0.06      0.10       481\n",
      "         7.0       0.19      0.04      0.07       612\n",
      "         8.0       0.26      0.12      0.16      1048\n",
      "         9.0       0.27      0.14      0.18      1424\n",
      "        10.0       0.23      0.76      0.35      1892\n",
      "\n",
      "   micro avg       0.23      0.23      0.23      9609\n",
      "   macro avg       0.25      0.15      0.13      9609\n",
      "weighted avg       0.24      0.23      0.17      9609\n",
      "\n",
      "0.23020085336663546\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(ytest,clf_RF_predictions))  \n",
    "print(classification_report(ytest,clf_RF_predictions))  \n",
    "print(accuracy_score(ytest, clf_RF_predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2560099906337808"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "clf_RF.score(x, y, sample_weight=None)  # on the entire data here so obviously it goes up by a few percentages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are performing ngram(1,2): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed text vector is \n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 1]\n",
      " ...\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 1]]\n",
      "Words for each feature:\n",
      "['absolut', 'acn', 'actual', 'ago', 'almost', 'also', 'alway', 'anoth', 'anxieti', 'anyth', 'around', 'away', 'back', 'bad', 'bc', 'best', 'better', 'birth', 'birth control', 'bit', 'bleed', 'bloat', 'bodi', 'break', 'breast', 'cant', 'caus', 'chang', 'clear', 'come', 'complet', 'constant', 'control', 'could', 'coupl', 'cramp', 'crazi', 'cri', 'day', 'decid', 'definit', 'depress', 'didnt', 'differ', 'doctor', 'dont', 'drive', 'due', 'eat', 'effect', 'emot', 'end', 'even', 'ever', 'everi', 'everyon', 'everyth', 'experi', 'experienc', 'extrem', 'face', 'far', 'feel', 'feel like', 'felt', 'fine', 'first', 'first month', 'gain', 'gain weight', 'get', 'give', 'go', 'good', 'got', 'gotten', 'great', 'half', 'happi', 'havent', 'headach', 'heavi', 'help', 'high', 'hope', 'hormon', 'horribl', 'hour', 'howev', 'hurt', 'im', 'implanon', 'implant', 'increas', 'insert', 'issu', 'iud', 'ive', 'know', 'last', 'lbs', 'life', 'light', 'like', 'littl', 'long', 'lost', 'lot', 'love', 'made', 'make', 'may', 'mirena', 'month', 'mood', 'mood swing', 'moodi', 'much', 'nausea', 'negat', 'never', 'nexplanon', 'next', 'normal', 'noth', 'notic', 'old', 'one', 'overal', 'pain', 'period', 'pill', 'pound', 'pregnanc', 'pregnant', 'pretti', 'problem', 'put', 'read', 'realli', 'reason', 'recommend', 'regular', 'remov', 'review', 'right', 'said', 'say', 'second', 'see', 'seem', 'sever', 'sex', 'sex drive', 'shot', 'side', 'side effect', 'sinc', 'skin', 'someth', 'spot', 'start', 'still', 'stop', 'sure', 'swing', 'switch', 'symptom', 'take', 'take pill', 'taken', 'terribl', 'that', 'thing', 'think', 'though', 'thought', 'three', 'time', 'told', 'took', 'tri', 'two', 'use', 'want', 'wasnt', 'way', 'week', 'weight', 'weight gain', 'well', 'went', 'work', 'worri', 'wors', 'worst', 'worth', 'would', 'year']\n"
     ]
    }
   ],
   "source": [
    "# So now, let's change the ngram to (1,2)\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2), min_df=0.05, max_df=1.0, max_features=500)\n",
    "\n",
    "# call `fit` to build the vocabulary\n",
    "vectorizer.fit(norm_corpus)\n",
    "\n",
    "# call `transform` to convert text to a bag of words\n",
    "x = vectorizer.transform(norm_corpus)\n",
    "\n",
    "# CountVectorizer uses a sparse array to save memory, but it's easier in this assignment to \n",
    "# convert back to a \"normal\" numpy array\n",
    "x = x.toarray()  # this is for visualization purposes \n",
    "\n",
    "print()\n",
    "print(\"Transformed text vector is \\n\", x)\n",
    "\n",
    "# `get_feature_names` tracks which word is associated with each column of the transformed x\n",
    "print\n",
    "print(\"Words for each feature:\")\n",
    "print(vectorizer.get_feature_names())  # visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38436, 199)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df2[df2['condition'] == 'Birth Control'].rating  # df2['rating']\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y)  #(default=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nazanin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    " #regressor target variables rating is \n",
    "# Train the model with Random Foprest Classifier     \n",
    "clf_RF = RandomForestClassifier()  \n",
    "#n_jobs=2 number of cores the computer uses, n_estimators = 500 (number of trees)  startt 50 \n",
    "clf_RF.fit(xtrain, ytrain)  \n",
    "# predict and evaluate performance\n",
    "clf_RF_predictions = clf_RF.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00261367 0.00780871 0.00252133 0.00427247 0.00475976 0.00751505\n",
      " 0.00422981 0.0029895  0.00387865 0.00303289 0.003107   0.00344073\n",
      " 0.00636469 0.00757787 0.00281829 0.00457    0.00369148 0.00854427\n",
      " 0.00801773 0.00296896 0.00851889 0.00289245 0.00459808 0.0031987\n",
      " 0.00480537 0.00408266 0.00405533 0.00489358 0.00357263 0.00328636\n",
      " 0.00311246 0.00455431 0.0087329  0.00345672 0.00323442 0.00871686\n",
      " 0.00280028 0.00386266 0.01123768 0.00280397 0.00291506 0.00748976\n",
      " 0.00576805 0.00413464 0.00574328 0.00648376 0.0046758  0.00281816\n",
      " 0.00275476 0.00655606 0.00385661 0.00273766 0.00479863 0.00475689\n",
      " 0.00606255 0.00273185 0.00266058 0.00506077 0.00473704 0.00443979\n",
      " 0.0030949  0.00570814 0.00755933 0.00275802 0.00418533 0.00295539\n",
      " 0.00919187 0.00404637 0.00882739 0.00310749 0.0119212  0.00311335\n",
      " 0.00710692 0.00543622 0.00855409 0.00340439 0.00508978 0.00253946\n",
      " 0.00307303 0.00487165 0.00479056 0.0044822  0.00452421 0.00231088\n",
      " 0.00449298 0.00381928 0.00534128 0.00272746 0.00457527 0.00290828\n",
      " 0.01154466 0.00331131 0.00373687 0.00340056 0.00828122 0.00348725\n",
      " 0.0029111  0.01160436 0.00345969 0.00612508 0.00370376 0.00371597\n",
      " 0.0048786  0.00817323 0.00509838 0.00349369 0.00280426 0.00477197\n",
      " 0.01380795 0.00530388 0.00431944 0.00245377 0.00340231 0.01513503\n",
      " 0.00605494 0.00550501 0.00351457 0.00512139 0.00354341 0.0023562\n",
      " 0.00750101 0.00365046 0.00282222 0.0050779  0.0038674  0.00574151\n",
      " 0.00264576 0.00715693 0.00395973 0.00804222 0.01571289 0.01247417\n",
      " 0.00456546 0.00451589 0.00553238 0.00345524 0.00461654 0.00498929\n",
      " 0.00303574 0.00641965 0.00273565 0.00576549 0.00306961 0.00554386\n",
      " 0.00364657 0.00255656 0.00274459 0.00388151 0.00337483 0.0034375\n",
      " 0.00289925 0.00446678 0.00646872 0.00393605 0.00316977 0.00600459\n",
      " 0.00517476 0.00784494 0.00395214 0.00293092 0.0077898  0.01000613\n",
      " 0.00499929 0.00667765 0.00299282 0.00533911 0.00557841 0.00282211\n",
      " 0.01022309 0.00391472 0.00305477 0.00377784 0.00305486 0.0053894\n",
      " 0.00473424 0.00343943 0.00339598 0.00342066 0.00900471 0.00258181\n",
      " 0.00415965 0.00589717 0.00594683 0.007501   0.00464417 0.00316699\n",
      " 0.00232613 0.01135039 0.00814229 0.00499299 0.00416511 0.00479723\n",
      " 0.00632275 0.00292573 0.00350533 0.005576   0.00296324 0.00730462\n",
      " 0.00887159]\n"
     ]
    }
   ],
   "source": [
    "print(clf_RF.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "print(clf_RF.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 'auto', 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "#gridsearch and hyperparameter tuning (on the TRAIN set) #THIS CELL IS 24K GOLD! \n",
    "param_grid = {'n_estimators': [100, 200, 300, 500], 'max_features': ['auto', None, 'log2']}\n",
    "clf_RF = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "clf_RF.fit(xtrain, ytrain)\n",
    "print(clf_RF.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 'auto', 'n_estimators': 100} 0.2268 0.0033\n",
      "{'max_features': 'auto', 'n_estimators': 200} 0.2258 0.003\n",
      "{'max_features': 'auto', 'n_estimators': 300} 0.2267 0.0026\n",
      "{'max_features': 'auto', 'n_estimators': 500} 0.2268 0.003\n",
      "{'max_features': None, 'n_estimators': 100} 0.2269 0.0036\n",
      "{'max_features': None, 'n_estimators': 200} 0.2259 0.0028\n",
      "{'max_features': None, 'n_estimators': 300} 0.2266 0.0027\n",
      "{'max_features': None, 'n_estimators': 500} 0.2268 0.0028\n",
      "{'max_features': 'log2', 'n_estimators': 100} 0.2268 0.0033\n",
      "{'max_features': 'log2', 'n_estimators': 200} 0.2258 0.003\n",
      "{'max_features': 'log2', 'n_estimators': 300} 0.2267 0.0026\n",
      "{'max_features': 'log2', 'n_estimators': 500} 0.2268 0.003\n"
     ]
    }
   ],
   "source": [
    "results_12gram = clf_RF.cv_results_\n",
    "for param, score_mean, score_sd in zip(results['params'], results['mean_test_score'], results['std_test_score']):\n",
    "    print(param, round(score_mean, 4), round(score_sd, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RF = RandomForestClassifier(n_estimators=500, max_features='auto', random_state=42)\n",
    "clf_RF.fit(xtrain, ytrain)\n",
    "clf_RF_predictions = clf_RF.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1331    0    2    0    0    0    0    6   16   74]\n",
      " [ 137  525    1    0    4    0    0    8   10   35]\n",
      " [ 118    4  493    0    4    0    0    0   12   31]\n",
      " [  75    0    3  384    2    0    0    4   29   45]\n",
      " [  70    0    6    0  557    2    0    8   28   58]\n",
      " [  47    0    2    0    0  328    0   13   17   39]\n",
      " [  43    0    0    0    2    0  483   11   43   47]\n",
      " [  52    2    4    0    0    0    0  790   77  127]\n",
      " [  59    2    2    0    0    0    0    7 1176  261]\n",
      " [  25    0    0    0    0    0    0    6   54 1808]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.68      0.93      0.79      1429\n",
      "         2.0       0.98      0.73      0.84       720\n",
      "         3.0       0.96      0.74      0.84       662\n",
      "         4.0       1.00      0.71      0.83       542\n",
      "         5.0       0.98      0.76      0.86       729\n",
      "         6.0       0.99      0.74      0.85       446\n",
      "         7.0       1.00      0.77      0.87       629\n",
      "         8.0       0.93      0.75      0.83      1052\n",
      "         9.0       0.80      0.78      0.79      1507\n",
      "        10.0       0.72      0.96      0.82      1893\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      9609\n",
      "   macro avg       0.90      0.79      0.83      9609\n",
      "weighted avg       0.85      0.82      0.82      9609\n",
      "\n",
      "0.8195441773337496\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(ytest,clf_RF_predictions))  \n",
    "print(classification_report(ytest,clf_RF_predictions))  \n",
    "print(accuracy_score(ytest, clf_RF_predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8195441773337496"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "clf_RF.score(xtest, ytest, sample_weight=None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Depression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_corpus_D = normalize_corpus(df2[df2['condition'] == 'Depression'].review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('depress', 9308),\n",
       " ('take', 8613),\n",
       " ('feel', 8433),\n",
       " ('mg', 7995),\n",
       " ('day', 6832),\n",
       " ('effect', 6674),\n",
       " ('year', 5806),\n",
       " ('week', 5664),\n",
       " ('side', 5424),\n",
       " ('work', 5353),\n",
       " ('start', 5261),\n",
       " ('month', 4714),\n",
       " ('anxieti', 4674),\n",
       " ('im', 4345),\n",
       " ('like', 4249),\n",
       " ('help', 4242),\n",
       " ('medic', 3905),\n",
       " ('life', 3884),\n",
       " ('time', 3825),\n",
       " ('tri', 3736),\n",
       " ('ive', 3729),\n",
       " ('get', 3585),\n",
       " ('first', 3486),\n",
       " ('felt', 3143),\n",
       " ('better', 3143),\n",
       " ('sleep', 2807),\n",
       " ('go', 2745),\n",
       " ('doctor', 2734),\n",
       " ('medicin', 2708),\n",
       " ('back', 2593),\n",
       " ('much', 2483),\n",
       " ('would', 2421),\n",
       " ('also', 2418),\n",
       " ('one', 2203),\n",
       " ('realli', 2192),\n",
       " ('drug', 2190),\n",
       " ('good', 2128),\n",
       " ('dose', 2090),\n",
       " ('still', 2037),\n",
       " ('weight', 1999)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n_words(normalize_corpus(df2[df2['condition'] == 'Depression'].review), n=40)  #None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed text vector is \n",
      " [[0 1 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Words for each feature:\n",
      "['abl', 'actual', 'ago', 'almost', 'also', 'alway', 'antidepress', 'anxieti', 'anyth', 'appetit', 'attack', 'away', 'back', 'bad', 'bed', 'best', 'better', 'cant', 'caus', 'celexa', 'chang', 'come', 'complet', 'could', 'couldnt', 'coupl', 'cri', 'cymbalta', 'day', 'depress', 'depress anxieti', 'didnt', 'differ', 'disord', 'doctor', 'dont', 'dose', 'drive', 'drug', 'due', 'eat', 'effect', 'effexor', 'energi', 'even', 'ever', 'everi', 'everyth', 'experi', 'experienc', 'extrem', 'far', 'feel', 'feel like', 'felt', 'final', 'first', 'first week', 'found', 'gain', 'gave', 'get', 'give', 'go', 'gone', 'good', 'got', 'great', 'happi', 'headach', 'help', 'high', 'hope', 'horribl', 'hour', 'howev', 'im', 'improv', 'increas', 'insomnia', 'issu', 'ive', 'know', 'last', 'less', 'lexapro', 'life', 'like', 'littl', 'long', 'lost', 'lot', 'love', 'made', 'major', 'make', 'mani', 'med', 'medic', 'medicin', 'mg', 'month', 'mood', 'morn', 'much', 'much better', 'nausea', 'need', 'never', 'night', 'normal', 'noth', 'notic', 'old', 'one', 'pain', 'panic', 'past', 'peopl', 'person', 'pill', 'posit', 'prescrib', 'pristiq', 'problem', 'prozac', 'put', 'quit', 'realli', 'recommend', 'say', 'see', 'seem', 'sever', 'sex', 'side', 'side effect', 'sinc', 'sleep', 'someth', 'start', 'start mg', 'start take', 'still', 'stop', 'suffer', 'suicid', 'switch', 'symptom', 'take', 'take mg', 'taken', 'thing', 'think', 'though', 'thought', 'time', 'tire', 'took', 'tri', 'two', 'use', 'want', 'way', 'week', 'weight', 'weight gain', 'well', 'wellbutrin', 'went', 'withdraw', 'within', 'without', 'wonder', 'work', 'wors', 'would', 'year', 'zoloft']\n"
     ]
    }
   ],
   "source": [
    "# So now, let's change the ngram to (1,2)\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2), min_df=0.05, max_df=1.0, max_features=500)\n",
    "\n",
    "# call `fit` to build the vocabulary\n",
    "vectorizer.fit(norm_corpus_D)\n",
    "\n",
    "# call `transform` to convert text to a bag of words\n",
    "x = vectorizer.transform(norm_corpus_D)\n",
    "\n",
    "# CountVectorizer uses a sparse array to save memory, but it's easier in this assignment to \n",
    "# convert back to a \"normal\" numpy array\n",
    "x = x.toarray()  # this is for visualization purposes \n",
    "\n",
    "print()\n",
    "print(\"Transformed text vector is \\n\", x)\n",
    "\n",
    "# `get_feature_names` tracks which word is associated with each column of the transformed x\n",
    "print\n",
    "print(\"Words for each feature:\")\n",
    "print(vectorizer.get_feature_names())  # visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df2[df2['condition'] == 'Depression'].rating  # df2['rating']\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y)  #(default=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nazanin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#regressor target variables rating is \n",
    "# Train the model with Random Foprest Classifier     \n",
    "clf_RF = RandomForestClassifier()  \n",
    "#n_jobs=2 number of cores the computer uses, n_estimators = 500 (number of trees)  startt 50 \n",
    "clf_RF.fit(xtrain, ytrain)  \n",
    "# predict and evaluate performance\n",
    "clf_RF_predictions = clf_RF.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00403743 0.00293845 0.00584924 0.0043932  0.00726728 0.0037476\n",
      " 0.0058153  0.0103228  0.0029234  0.00326871 0.00354678 0.00469821\n",
      " 0.00707556 0.00591681 0.00247209 0.00372176 0.00802306 0.00377743\n",
      " 0.00521608 0.0038862  0.00550516 0.0037061  0.00290006 0.00502601\n",
      " 0.00365341 0.00319737 0.00378681 0.00458883 0.01356203 0.01266173\n",
      " 0.00334667 0.0064951  0.00565808 0.00246259 0.00773281 0.00653911\n",
      " 0.00732382 0.00351998 0.00826284 0.00356587 0.00332787 0.00981057\n",
      " 0.00443697 0.00479073 0.00532518 0.00477784 0.0042282  0.00277781\n",
      " 0.00441894 0.00505709 0.00514092 0.00411013 0.01590123 0.00470816\n",
      " 0.00729207 0.00353696 0.00836153 0.00343788 0.00337364 0.00590171\n",
      " 0.00287309 0.00949885 0.0036486  0.00836927 0.00295644 0.00701837\n",
      " 0.00513867 0.00570425 0.00460049 0.00455335 0.01161411 0.00270649\n",
      " 0.00471333 0.00422732 0.00349162 0.00473942 0.01123247 0.0044876\n",
      " 0.00618887 0.00431485 0.00349838 0.00957047 0.00424598 0.00465496\n",
      " 0.00477218 0.00593357 0.01279759 0.00973789 0.00515272 0.00330052\n",
      " 0.0038269  0.00618115 0.00304625 0.00671715 0.00368286 0.00579367\n",
      " 0.00493567 0.00459325 0.01022303 0.00883564 0.01464575 0.01118501\n",
      " 0.00770223 0.00391937 0.00797008 0.00220359 0.00672517 0.00423183\n",
      " 0.00490667 0.00661052 0.00401483 0.00392742 0.0058261  0.00278386\n",
      " 0.00712593 0.00351582 0.00350089 0.00239025 0.00338978 0.00336403\n",
      " 0.00364342 0.00360401 0.00468346 0.00490865 0.00474236 0.00585928\n",
      " 0.00515706 0.00328674 0.00855721 0.00291139 0.00412589 0.00462414\n",
      " 0.00441213 0.00596341 0.00320703 0.00768    0.00770312 0.00563797\n",
      " 0.0083548  0.00260372 0.01102815 0.00265024 0.00366382 0.00665637\n",
      " 0.00738685 0.00375337 0.00464009 0.00410085 0.00421582 0.01314685\n",
      " 0.00349115 0.00464887 0.00535638 0.00555195 0.002702   0.00547841\n",
      " 0.0079788  0.00423162 0.0064452  0.00836335 0.0051602  0.00560522\n",
      " 0.00422258 0.00355406 0.01209714 0.00638299 0.00308897 0.00762949\n",
      " 0.00607641 0.00538387 0.00360773 0.00356686 0.00237709 0.00299622\n",
      " 0.01328513 0.00677653 0.006736   0.01198943 0.00568454]\n"
     ]
    }
   ],
   "source": [
    "print(clf_RF.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "print(clf_RF.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 'log2', 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "#gridsearch and hyperparameter tuning (on the TRAIN set) #THIS CELL IS 24K GOLD! \n",
    "param_grid = {'n_estimators': [100, 200, 300, 500], 'max_features': ['auto', None, 'log2']}\n",
    "clf_RF = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "clf_RF.fit(xtrain, ytrain)\n",
    "print(clf_RF.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RF = RandomForestClassifier(n_estimators=200, max_features='log2', random_state=42)\n",
    "clf_RF.fit(xtrain, ytrain)\n",
    "clf_RF_predictions = clf_RF.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[290   1   0   0   0   0   0   3  18  56]\n",
      " [ 17  81   0   0   0   0   1   4   4  27]\n",
      " [ 10   1  72   0   1   0   0   1   0  32]\n",
      " [  2   0   0  67   0   0   0   1  12  13]\n",
      " [  4   0   0   0  82   0   0   6   8  30]\n",
      " [  5   0   0   0   0  87   0   8   8  26]\n",
      " [  6   0   0   0   0   1 140  11  14  53]\n",
      " [  7   0   0   2   0   0   0 251  34 105]\n",
      " [  5   0   0   0   0   1   2   8 401 184]\n",
      " [ 11   0   0   0   1   1   0  16  31 778]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.81      0.79      0.80       368\n",
      "         2.0       0.98      0.60      0.75       134\n",
      "         3.0       1.00      0.62      0.76       117\n",
      "         4.0       0.97      0.71      0.82        95\n",
      "         5.0       0.98      0.63      0.77       130\n",
      "         6.0       0.97      0.65      0.78       134\n",
      "         7.0       0.98      0.62      0.76       225\n",
      "         8.0       0.81      0.63      0.71       399\n",
      "         9.0       0.76      0.67      0.71       601\n",
      "        10.0       0.60      0.93      0.73       838\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      3041\n",
      "   macro avg       0.88      0.68      0.76      3041\n",
      "weighted avg       0.79      0.74      0.74      3041\n",
      "\n",
      "0.7395593554751726\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(ytest,clf_RF_predictions))  \n",
    "print(classification_report(ytest,clf_RF_predictions))  \n",
    "print(accuracy_score(ytest, clf_RF_predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed text vector is \n",
      " [[0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Words for each feature:\n",
      "['depress anxieti', 'feel like', 'first week', 'much better', 'side effect', 'start mg', 'start take', 'take mg', 'weight gain']\n"
     ]
    }
   ],
   "source": [
    "# So now, let's change the ngram to (1,2)\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2), min_df=0.05, max_df=1.0, max_features=500)\n",
    "# call `fit` to build the vocabulary\n",
    "vectorizer.fit(norm_corpus_D)\n",
    "# call `transform` to convert text to a bag of words\n",
    "x = vectorizer.transform(norm_corpus_D)\n",
    "# CountVectorizer uses a sparse array to save memory, but it's easier in this assignment to \n",
    "# convert back to a \"normal\" numpy array\n",
    "x = x.toarray()  # this is for visualization purposes \n",
    "print()\n",
    "print(\"Transformed text vector is \\n\", x)\n",
    "# `get_feature_names` tracks which word is associated with each column of the transformed x\n",
    "print\n",
    "print(\"Words for each feature:\")\n",
    "print(vectorizer.get_feature_names())  # visualization \n",
    "y = df2[df2['condition'] == 'Depression'].rating  # df2['rating']\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y)  #(default=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nazanin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#regressor target variables rating is \n",
    "# Train the model with Random Foprest Classifier     \n",
    "clf_RF = RandomForestClassifier()  \n",
    "#n_jobs=2 number of cores the computer uses, n_estimators = 500 (number of trees)  startt 50 \n",
    "clf_RF.fit(xtrain, ytrain)  \n",
    "# predict and evaluate performance\n",
    "clf_RF_predictions = clf_RF.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09455699 0.12580011 0.09292928 0.07954719 0.15748121 0.09520075\n",
      " 0.13021056 0.11401512 0.1102588 ]\n"
     ]
    }
   ],
   "source": [
    "print(clf_RF.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "print(clf_RF.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': None, 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "#gridsearch and hyperparameter tuning (on the TRAIN set) #THIS CELL IS 24K GOLD! \n",
    "param_grid = {'n_estimators': [50, 100, 200, 300, 500], 'max_features': ['auto', None, 'log2']}\n",
    "clf_RF = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "clf_RF.fit(xtrain, ytrain)\n",
    "print(clf_RF.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RF = RandomForestClassifier(n_estimators=300, max_features=None, random_state=42)\n",
    "clf_RF.fit(xtrain, ytrain)\n",
    "clf_RF_predictions = clf_RF.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5   0   0   0   0   0   3   3  22 324]\n",
      " [  0   2   0   0   0   2   2   3   9 125]\n",
      " [  0   0   2   1   0   0   0   6  10  92]\n",
      " [  2   0   1   1   0   0   0   5   8  72]\n",
      " [  0   0   0   1   6   0   0   6  17 118]\n",
      " [  3   0   0   0   0   3   1   8  13 127]\n",
      " [  3   0   0   0   1   1   8  11  20 172]\n",
      " [  0   0   1   0   1   0   1  33  47 312]\n",
      " [  5   1   5   0   1   0   1  21  85 463]\n",
      " [  5   2   2   1   3   1   2  32  90 707]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.22      0.01      0.03       357\n",
      "         2.0       0.40      0.01      0.03       143\n",
      "         3.0       0.18      0.02      0.03       111\n",
      "         4.0       0.25      0.01      0.02        89\n",
      "         5.0       0.50      0.04      0.07       148\n",
      "         6.0       0.43      0.02      0.04       155\n",
      "         7.0       0.44      0.04      0.07       216\n",
      "         8.0       0.26      0.08      0.13       395\n",
      "         9.0       0.26      0.15      0.19       582\n",
      "        10.0       0.28      0.84      0.42       845\n",
      "\n",
      "   micro avg       0.28      0.28      0.28      3041\n",
      "   macro avg       0.32      0.12      0.10      3041\n",
      "weighted avg       0.30      0.28      0.19      3041\n",
      "\n",
      "0.2801709963827688\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(ytest,clf_RF_predictions))  \n",
    "print(classification_report(ytest,clf_RF_predictions))  \n",
    "print(accuracy_score(ytest, clf_RF_predictions)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_corpus_P = normalize_corpus(df2[df2['condition'] == 'Pain'].review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pain', 13862),\n",
       " ('take', 5484),\n",
       " ('work', 4136),\n",
       " ('day', 3902),\n",
       " ('mg', 3545),\n",
       " ('back', 2829),\n",
       " ('year', 2763),\n",
       " ('effect', 2486),\n",
       " ('help', 2341),\n",
       " ('time', 2181),\n",
       " ('medicin', 2140),\n",
       " ('doctor', 2102),\n",
       " ('medic', 1990),\n",
       " ('get', 1960),\n",
       " ('use', 1842),\n",
       " ('sever', 1804),\n",
       " ('side', 1801),\n",
       " ('hour', 1687),\n",
       " ('relief', 1557),\n",
       " ('tri', 1507),\n",
       " ('prescrib', 1507),\n",
       " ('feel', 1490),\n",
       " ('like', 1410),\n",
       " ('im', 1384),\n",
       " ('well', 1363),\n",
       " ('surgeri', 1316),\n",
       " ('ive', 1256),\n",
       " ('one', 1245),\n",
       " ('life', 1240),\n",
       " ('start', 1216),\n",
       " ('month', 1209),\n",
       " ('would', 1199),\n",
       " ('everi', 1171),\n",
       " ('also', 1147),\n",
       " ('patch', 1130),\n",
       " ('great', 1124),\n",
       " ('go', 1063),\n",
       " ('much', 1063),\n",
       " ('need', 1058),\n",
       " ('better', 1023)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n_words(normalize_corpus(df2[df2['condition'] == 'Pain'].review), n=40)  #None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed text vector is \n",
      " [[0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 1 0 ... 0 0 4]\n",
      " [0 0 0 ... 0 1 1]\n",
      " [0 0 0 ... 0 0 1]]\n",
      "Words for each feature:\n",
      "['abl', 'ago', 'almost', 'also', 'away', 'back', 'back pain', 'bad', 'best', 'better', 'caus', 'chronic', 'chronic pain', 'could', 'daili', 'day', 'didnt', 'differ', 'disc', 'doctor', 'dont', 'dose', 'drug', 'due', 'effect', 'er', 'even', 'everi', 'feel', 'felt', 'first', 'found', 'gave', 'get', 'give', 'given', 'go', 'good', 'got', 'great', 'headach', 'help', 'hour', 'im', 'ive', 'knee', 'last', 'leg', 'life', 'like', 'littl', 'long', 'lot', 'lower', 'made', 'make', 'manag', 'mani', 'med', 'medic', 'medicin', 'mg', 'migrain', 'month', 'much', 'neck', 'need', 'nerv', 'never', 'night', 'norco', 'noth', 'one', 'oxycontin', 'pain', 'pain relief', 'patch', 'percocet', 'pill', 'prescrib', 'problem', 'put', 'realli', 'relief', 'reliev', 'say', 'seem', 'sever', 'side', 'side effect', 'sinc', 'sleep', 'start', 'still', 'stop', 'suffer', 'surgeri', 'take', 'take mg', 'taken', 'thing', 'time', 'took', 'tramadol', 'tri', 'two', 'use', 'vicodin', 'week', 'well', 'went', 'without', 'wonder', 'work', 'work well', 'would', 'year']\n"
     ]
    }
   ],
   "source": [
    "# So now, let's change the ngram to (1,2)\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2), min_df=0.05, max_df=1.0, max_features=500)\n",
    "\n",
    "# call `fit` to build the vocabulary\n",
    "vectorizer.fit(norm_corpus_P)\n",
    "\n",
    "# call `transform` to convert text to a bag of words\n",
    "x = vectorizer.transform(norm_corpus_P)\n",
    "\n",
    "# CountVectorizer uses a sparse array to save memory, but it's easier in this assignment to \n",
    "# convert back to a \"normal\" numpy array\n",
    "x = x.toarray()  # this is for visualization purposes \n",
    "\n",
    "print()\n",
    "print(\"Transformed text vector is \\n\", x)\n",
    "\n",
    "# `get_feature_names` tracks which word is associated with each column of the transformed x\n",
    "print\n",
    "print(\"Words for each feature:\")\n",
    "print(vectorizer.get_feature_names())  # visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df2[df2['condition'] == 'Pain'].rating  # df2['rating']\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y)  #(default=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nazanin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#regressor target variables rating is \n",
    "# Train the model with Random Foprest Classifier     \n",
    "clf_RF = RandomForestClassifier()  \n",
    "#n_jobs=2 number of cores the computer uses, n_estimators = 500 (number of trees)  startt 50 \n",
    "clf_RF.fit(xtrain, ytrain)  \n",
    "# predict and evaluate performance\n",
    "clf_RF_predictions = clf_RF.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00507205 0.00462299 0.00565628 0.00893089 0.00652562 0.01489724\n",
      " 0.00703229 0.00728882 0.0065098  0.00914574 0.0070263  0.00687031\n",
      " 0.00419252 0.00536318 0.00547727 0.0167282  0.00662916 0.0054591\n",
      " 0.00761032 0.01218975 0.0097575  0.00781779 0.00843896 0.00676684\n",
      " 0.01551271 0.00752545 0.00791305 0.00759387 0.01159906 0.00437626\n",
      " 0.00819048 0.00639034 0.00616641 0.01327115 0.00702463 0.0083925\n",
      " 0.00916835 0.00951327 0.00583812 0.01149034 0.00604197 0.01701391\n",
      " 0.01266354 0.00929397 0.01067413 0.00582213 0.00806199 0.00540672\n",
      " 0.00840218 0.01186835 0.00806511 0.00644472 0.00619722 0.0063577\n",
      " 0.00661712 0.01046372 0.00500311 0.00594504 0.00537888 0.01479124\n",
      " 0.01421884 0.01584166 0.00620655 0.00908773 0.00986737 0.00495396\n",
      " 0.00861364 0.00592331 0.00578949 0.00685851 0.00593137 0.00736894\n",
      " 0.00947436 0.00465462 0.03419348 0.00570851 0.00894881 0.00645307\n",
      " 0.00622186 0.01121615 0.00573276 0.00625843 0.00821843 0.01172088\n",
      " 0.00724977 0.00439848 0.00673739 0.01312556 0.01065716 0.00829151\n",
      " 0.00525165 0.00882831 0.0104506  0.00840562 0.00658201 0.00528976\n",
      " 0.00981868 0.01931488 0.00394515 0.00610665 0.00553772 0.01243154\n",
      " 0.00864706 0.00637361 0.01028159 0.00601469 0.01383049 0.00763143\n",
      " 0.00855963 0.00882678 0.00516846 0.00690256 0.00443991 0.01875427\n",
      " 0.00408768 0.00911964 0.01298944]\n"
     ]
    }
   ],
   "source": [
    "print(clf_RF.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "print(clf_RF.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 'log2', 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "#gridsearch and hyperparameter tuning (on the TRAIN set) #THIS CELL IS 24K GOLD! \n",
    "param_grid = {'n_estimators': [50, 100, 200, 300, 500], 'max_features': ['auto', None, 'log2']}\n",
    "clf_RF = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "clf_RF.fit(xtrain, ytrain)\n",
    "print(clf_RF.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RF = RandomForestClassifier(n_estimators=300, max_features='log2', random_state=42)\n",
    "clf_RF.fit(xtrain, ytrain)\n",
    "clf_RF_predictions = clf_RF.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[115   2   0   0   0   1   4   2  12  58]\n",
      " [  0  33   0   0   0   0   2   5   5  14]\n",
      " [  3   0  25   0   0   0   0   2   6  14]\n",
      " [  2   0   0  31   0   0   0   1   4  11]\n",
      " [  2   0   0   1  41   0   0   0   3  19]\n",
      " [  2   0   0   0   0  28   2   1   3  25]\n",
      " [  5   0   0   0   0   0  63   6   8  42]\n",
      " [  5   0   0   2   0   0   0 178  31 103]\n",
      " [  4   0   2   0   1   2   1   9 262 162]\n",
      " [  4   2   0   1   3   2   2  13  30 640]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.81      0.59      0.68       194\n",
      "         2.0       0.89      0.56      0.69        59\n",
      "         3.0       0.93      0.50      0.65        50\n",
      "         4.0       0.89      0.63      0.74        49\n",
      "         5.0       0.91      0.62      0.74        66\n",
      "         6.0       0.85      0.46      0.60        61\n",
      "         7.0       0.85      0.51      0.64       124\n",
      "         8.0       0.82      0.56      0.66       319\n",
      "         9.0       0.72      0.59      0.65       443\n",
      "        10.0       0.59      0.92      0.72       697\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      2062\n",
      "   macro avg       0.83      0.59      0.68      2062\n",
      "weighted avg       0.73      0.69      0.68      2062\n",
      "\n",
      "0.6867119301648884\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(ytest,clf_RF_predictions))  \n",
    "print(classification_report(ytest,clf_RF_predictions))  \n",
    "print(accuracy_score(ytest, clf_RF_predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization complete: \n",
      "Vectorization complete: \n",
      "creating train test data: \n",
      "Training model: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nazanin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished: Birth Control\n",
      "[[1257   32   25   10   26    6    8   13    4   27]\n",
      " [  95  518   14    8    6    2    6   10    6   16]\n",
      " [  91   10  476    6   20    8    4    5   18   19]\n",
      " [  53    6    9  403    7    4    4   10    5   21]\n",
      " [  66    6   17   15  532    4    9   21   32   36]\n",
      " [  40    6   14    9   12  363    7   10   24   31]\n",
      " [  48    6    6    9    8    3  468   18   15   18]\n",
      " [  48   10    8    9   15    2   10  821   54   73]\n",
      " [  51   10   11    5    6   14   19   40 1165  170]\n",
      " [  75   13   17    8   18    4   17   43   95 1657]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.69      0.89      0.78      1408\n",
      "         2.0       0.84      0.76      0.80       681\n",
      "         3.0       0.80      0.72      0.76       657\n",
      "         4.0       0.84      0.77      0.80       522\n",
      "         5.0       0.82      0.72      0.77       738\n",
      "         6.0       0.89      0.70      0.78       516\n",
      "         7.0       0.85      0.78      0.81       599\n",
      "         8.0       0.83      0.78      0.80      1050\n",
      "         9.0       0.82      0.78      0.80      1491\n",
      "        10.0       0.80      0.85      0.83      1947\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      9609\n",
      "   macro avg       0.82      0.78      0.79      9609\n",
      "weighted avg       0.80      0.80      0.80      9609\n",
      "\n",
      "0.7971693204287646\n",
      "Normalization complete: \n",
      "Vectorization complete: \n",
      "creating train test data: \n",
      "Training model: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nazanin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished: Depression\n",
      "[[285   9   3   1   5   3   0  17  20  33]\n",
      " [ 18  86   2   2   0   4   5   1   9   7]\n",
      " [ 17   0  73   2   1   3   0   7  14  10]\n",
      " [ 10   1   0  52   3   0   3   2   5   5]\n",
      " [ 11   1   1   1  82   1   1   9  14   6]\n",
      " [ 10   0   0   0   0  83   0   8   7  18]\n",
      " [  7   0   4   4   0   2 144  10  29  21]\n",
      " [ 19   3   2   3   1   4  11 287  48  50]\n",
      " [ 33   5   6   3   0   3   9  33 408  81]\n",
      " [ 34   2   1   1   3   5  10  33  82 669]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.64      0.76      0.70       376\n",
      "         2.0       0.80      0.64      0.71       134\n",
      "         3.0       0.79      0.57      0.67       127\n",
      "         4.0       0.75      0.64      0.69        81\n",
      "         5.0       0.86      0.65      0.74       127\n",
      "         6.0       0.77      0.66      0.71       126\n",
      "         7.0       0.79      0.65      0.71       221\n",
      "         8.0       0.71      0.67      0.69       428\n",
      "         9.0       0.64      0.70      0.67       581\n",
      "        10.0       0.74      0.80      0.77       840\n",
      "\n",
      "   micro avg       0.71      0.71      0.71      3041\n",
      "   macro avg       0.75      0.67      0.71      3041\n",
      "weighted avg       0.72      0.71      0.71      3041\n",
      "\n",
      "0.713252219664584\n",
      "Normalization complete: \n",
      "Vectorization complete: \n",
      "creating train test data: \n",
      "Training model: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nazanin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished: Pain\n",
      "[[121   4   1   1   1   1   3  11  11  31]\n",
      " [  2  32   0   0   0   0   2   4  10  12]\n",
      " [  6   1  30   0   0   0   0   7   1   8]\n",
      " [  5   2   0  20   1   0   0   4   5   7]\n",
      " [  4   0   0   0  31   1   1   3   4  15]\n",
      " [  6   0   0   0   0  31   1   6   4  10]\n",
      " [  2   2   0   0   3   1  66  14  11  19]\n",
      " [ 12   3   1   0   1   3  10 187  39  49]\n",
      " [ 10   0   1   1   2   2   8  34 288  96]\n",
      " [ 30   2   2   0   1   1  13  43  88 556]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.61      0.65      0.63       185\n",
      "         2.0       0.70      0.52      0.59        62\n",
      "         3.0       0.86      0.57      0.68        53\n",
      "         4.0       0.91      0.45      0.61        44\n",
      "         5.0       0.78      0.53      0.63        59\n",
      "         6.0       0.78      0.53      0.63        58\n",
      "         7.0       0.63      0.56      0.59       118\n",
      "         8.0       0.60      0.61      0.61       305\n",
      "         9.0       0.62      0.65      0.64       442\n",
      "        10.0       0.69      0.76      0.72       736\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      2062\n",
      "   macro avg       0.72      0.58      0.63      2062\n",
      "weighted avg       0.67      0.66      0.66      2062\n",
      "\n",
      "0.6605237633365665\n",
      "Normalization complete: \n",
      "Vectorization complete: \n",
      "creating train test data: \n",
      "Training model: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nazanin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished: Anxiety\n",
      "[[126   4   0   0   1   4   0  10  12  30]\n",
      " [  6  31   1   0   2   0   0   0   4   6]\n",
      " [  6   1  24   0   0   0   0   6   9   9]\n",
      " [  4   2   0  19   2   2   0   4  13   9]\n",
      " [  3   0   0   0  26   0   1   8   7  17]\n",
      " [  3   0   0   0   0  25   0   2   3  10]\n",
      " [  5   0   0   0   0   0  66   5   8  21]\n",
      " [ 16   1   1   1   1   0   2 151  31  57]\n",
      " [ 16   1   2   0   2   1   2  24 231 107]\n",
      " [ 12   3   2   1   0   2   3  39  66 621]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.64      0.67      0.66       187\n",
      "         2.0       0.72      0.62      0.67        50\n",
      "         3.0       0.80      0.44      0.56        55\n",
      "         4.0       0.90      0.35      0.50        55\n",
      "         5.0       0.76      0.42      0.54        62\n",
      "         6.0       0.74      0.58      0.65        43\n",
      "         7.0       0.89      0.63      0.74       105\n",
      "         8.0       0.61      0.58      0.59       261\n",
      "         9.0       0.60      0.60      0.60       386\n",
      "        10.0       0.70      0.83      0.76       749\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      1953\n",
      "   macro avg       0.74      0.57      0.63      1953\n",
      "weighted avg       0.68      0.68      0.67      1953\n",
      "\n",
      "0.6758832565284179\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "accuracies = {}\n",
    "#model = train_model(x,y)\n",
    "list_of_conditions = ['Birth Control', 'Depression', 'Pain', 'Anxiety']\n",
    "for condition in list_of_conditions:\n",
    "    norm_corpus = normalize_corpus(df2[df2['condition'] == condition].review)\n",
    "    print(\"Normalization complete: \")\n",
    "    vectorizer_loop = CountVectorizer(ngram_range=(1,2), min_df=0.05, max_df=1.0, max_features=500)\n",
    "    #x = vectorizer(normalize_corpus)\n",
    "    x = vectorizer_loop.fit_transform(norm_corpus)\n",
    "    print(\"Vectorization complete: \")\n",
    "    clf_RF_loop = RandomForestClassifier()\n",
    "    #x = vectorizer.transform(normalize_corpus)\n",
    "    y = df2[df2['condition']==condition]['rating']\n",
    "    print(\"creating train test data: \")\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x, y)  #(default=0.25) 4 objects are created and returend and we assign the 4 objects\n",
    "    # left side is an object \n",
    "    print(\"Training model: \")\n",
    "    model = clf_RF_loop.fit(xtrain,ytrain) #clf_RF.fit  train_model \n",
    "    models[condition]=model\n",
    "    clf_RF_loop_predictions = clf_RF_loop.predict(xtest)\n",
    "    accuracies[condition] =  confusion_matrix(ytest,clf_RF_loop_predictions)\n",
    "    print(\"finished: \" + condition)\n",
    "    print(confusion_matrix(ytest,clf_RF_loop_predictions))  \n",
    "    print(classification_report(ytest,clf_RF_loop_predictions))  \n",
    "    print(accuracy_score(ytest, clf_RF_loop_predictions)) \n",
    "\n",
    "#clf_RF.fit(xtrain, ytrain)\n",
    " #   clf_RF_predictions = clf_RF.predict(xtest)\n",
    "\n",
    "\n",
    "#accuracies = {}\n",
    "#accuracies[condition] = rsquared(model)\n",
    "#norm_corpus = normalize_corpus(df2[df2['condition'] == 'Birth Control'].review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imp</th>\n",
       "      <th>vars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002505</td>\n",
       "      <td>absolut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007727</td>\n",
       "      <td>acn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002709</td>\n",
       "      <td>actual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004506</td>\n",
       "      <td>ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005318</td>\n",
       "      <td>almost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.007333</td>\n",
       "      <td>also</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.004046</td>\n",
       "      <td>alway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.002746</td>\n",
       "      <td>anoth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.003640</td>\n",
       "      <td>anxieti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.002858</td>\n",
       "      <td>anyth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.003068</td>\n",
       "      <td>around</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.003485</td>\n",
       "      <td>away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.006176</td>\n",
       "      <td>back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.007039</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.003223</td>\n",
       "      <td>bc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.004238</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.003581</td>\n",
       "      <td>better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.008370</td>\n",
       "      <td>birth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.008033</td>\n",
       "      <td>birth control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.003159</td>\n",
       "      <td>bit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.008675</td>\n",
       "      <td>bleed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.003287</td>\n",
       "      <td>bloat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.004470</td>\n",
       "      <td>bodi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.002805</td>\n",
       "      <td>break</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.004591</td>\n",
       "      <td>breast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.003839</td>\n",
       "      <td>cant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.003991</td>\n",
       "      <td>caus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.004910</td>\n",
       "      <td>chang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.003724</td>\n",
       "      <td>clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.003229</td>\n",
       "      <td>come</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0.003912</td>\n",
       "      <td>take pill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0.002998</td>\n",
       "      <td>taken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.003872</td>\n",
       "      <td>terribl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0.002814</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.005382</td>\n",
       "      <td>thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0.004667</td>\n",
       "      <td>think</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.003331</td>\n",
       "      <td>though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.003395</td>\n",
       "      <td>thought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.003487</td>\n",
       "      <td>three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.008989</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.002543</td>\n",
       "      <td>told</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.004321</td>\n",
       "      <td>took</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.005699</td>\n",
       "      <td>tri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.005887</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0.007316</td>\n",
       "      <td>use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.004615</td>\n",
       "      <td>want</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.002999</td>\n",
       "      <td>wasnt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.002457</td>\n",
       "      <td>way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.010755</td>\n",
       "      <td>week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.008108</td>\n",
       "      <td>weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.004686</td>\n",
       "      <td>weight gain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.004183</td>\n",
       "      <td>well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.004949</td>\n",
       "      <td>went</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.006360</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.002964</td>\n",
       "      <td>worri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0.003352</td>\n",
       "      <td>wors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.005829</td>\n",
       "      <td>worst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.002693</td>\n",
       "      <td>worth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.007349</td>\n",
       "      <td>would</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.010417</td>\n",
       "      <td>year</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          imp           vars\n",
       "0    0.002505        absolut\n",
       "1    0.007727            acn\n",
       "2    0.002709         actual\n",
       "3    0.004506            ago\n",
       "4    0.005318         almost\n",
       "5    0.007333           also\n",
       "6    0.004046          alway\n",
       "7    0.002746          anoth\n",
       "8    0.003640        anxieti\n",
       "9    0.002858          anyth\n",
       "10   0.003068         around\n",
       "11   0.003485           away\n",
       "12   0.006176           back\n",
       "13   0.007039            bad\n",
       "14   0.003223             bc\n",
       "15   0.004238           best\n",
       "16   0.003581         better\n",
       "17   0.008370          birth\n",
       "18   0.008033  birth control\n",
       "19   0.003159            bit\n",
       "20   0.008675          bleed\n",
       "21   0.003287          bloat\n",
       "22   0.004470           bodi\n",
       "23   0.002805          break\n",
       "24   0.004591         breast\n",
       "25   0.003839           cant\n",
       "26   0.003991           caus\n",
       "27   0.004910          chang\n",
       "28   0.003724          clear\n",
       "29   0.003229           come\n",
       "..        ...            ...\n",
       "169  0.003912      take pill\n",
       "170  0.002998          taken\n",
       "171  0.003872        terribl\n",
       "172  0.002814           that\n",
       "173  0.005382          thing\n",
       "174  0.004667          think\n",
       "175  0.003331         though\n",
       "176  0.003395        thought\n",
       "177  0.003487          three\n",
       "178  0.008989           time\n",
       "179  0.002543           told\n",
       "180  0.004321           took\n",
       "181  0.005699            tri\n",
       "182  0.005887            two\n",
       "183  0.007316            use\n",
       "184  0.004615           want\n",
       "185  0.002999          wasnt\n",
       "186  0.002457            way\n",
       "187  0.010755           week\n",
       "188  0.008108         weight\n",
       "189  0.004686    weight gain\n",
       "190  0.004183           well\n",
       "191  0.004949           went\n",
       "192  0.006360           work\n",
       "193  0.002964          worri\n",
       "194  0.003352           wors\n",
       "195  0.005829          worst\n",
       "196  0.002693          worth\n",
       "197  0.007349          would\n",
       "198  0.010417           year\n",
       "\n",
       "[199 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.DataFrame(data={'imp':clf_RF.feature_importances_,'vars':vectorizer.get_feature_names()})\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imp</th>\n",
       "      <th>vars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.014883</td>\n",
       "      <td>month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.014685</td>\n",
       "      <td>period</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.013692</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.012514</td>\n",
       "      <td>pill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.011939</td>\n",
       "      <td>ive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.011831</td>\n",
       "      <td>get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.011769</td>\n",
       "      <td>im</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.011203</td>\n",
       "      <td>day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.010755</td>\n",
       "      <td>week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.010417</td>\n",
       "      <td>year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.010215</td>\n",
       "      <td>take</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.009806</td>\n",
       "      <td>first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.009552</td>\n",
       "      <td>start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.009522</td>\n",
       "      <td>cramp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.008989</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.008786</td>\n",
       "      <td>got</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.008675</td>\n",
       "      <td>bleed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.008637</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.008476</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.008405</td>\n",
       "      <td>gain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.008370</td>\n",
       "      <td>birth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.008283</td>\n",
       "      <td>pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.008108</td>\n",
       "      <td>weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.008033</td>\n",
       "      <td>birth control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007727</td>\n",
       "      <td>acn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.007555</td>\n",
       "      <td>sinc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.007516</td>\n",
       "      <td>feel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.007484</td>\n",
       "      <td>depress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.007450</td>\n",
       "      <td>insert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.007349</td>\n",
       "      <td>would</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.002930</td>\n",
       "      <td>complet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.002887</td>\n",
       "      <td>face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.002882</td>\n",
       "      <td>end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.002882</td>\n",
       "      <td>reason</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.002876</td>\n",
       "      <td>crazi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.002858</td>\n",
       "      <td>anyth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.002839</td>\n",
       "      <td>half</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.002839</td>\n",
       "      <td>hurt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0.002814</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.002805</td>\n",
       "      <td>break</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.002802</td>\n",
       "      <td>decid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.002785</td>\n",
       "      <td>everyth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.002782</td>\n",
       "      <td>read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.002755</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.002749</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.002746</td>\n",
       "      <td>anoth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.002731</td>\n",
       "      <td>fine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.002710</td>\n",
       "      <td>everyon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002709</td>\n",
       "      <td>actual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.002703</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.002693</td>\n",
       "      <td>worth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.002692</td>\n",
       "      <td>hour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.002675</td>\n",
       "      <td>someth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.002553</td>\n",
       "      <td>old</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.002543</td>\n",
       "      <td>told</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.002537</td>\n",
       "      <td>may</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.002530</td>\n",
       "      <td>negat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002505</td>\n",
       "      <td>absolut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.002457</td>\n",
       "      <td>way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.002420</td>\n",
       "      <td>feel like</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          imp           vars\n",
       "113  0.014883          month\n",
       "130  0.014685         period\n",
       "108  0.013692           love\n",
       "131  0.012514           pill\n",
       "97   0.011939            ive\n",
       "70   0.011831            get\n",
       "90   0.011769             im\n",
       "38   0.011203            day\n",
       "187  0.010755           week\n",
       "198  0.010417           year\n",
       "168  0.010215           take\n",
       "66   0.009806          first\n",
       "161  0.009552          start\n",
       "35   0.009522          cramp\n",
       "178  0.008989           time\n",
       "74   0.008786            got\n",
       "20   0.008675          bleed\n",
       "32   0.008637        control\n",
       "103  0.008476           like\n",
       "68   0.008405           gain\n",
       "17   0.008370          birth\n",
       "129  0.008283           pain\n",
       "188  0.008108         weight\n",
       "18   0.008033  birth control\n",
       "1    0.007727            acn\n",
       "157  0.007555           sinc\n",
       "62   0.007516           feel\n",
       "41   0.007484        depress\n",
       "94   0.007450         insert\n",
       "197  0.007349          would\n",
       "..        ...            ...\n",
       "30   0.002930        complet\n",
       "60   0.002887           face\n",
       "51   0.002882            end\n",
       "140  0.002882         reason\n",
       "36   0.002876          crazi\n",
       "9    0.002858          anyth\n",
       "77   0.002839           half\n",
       "89   0.002839           hurt\n",
       "172  0.002814           that\n",
       "23   0.002805          break\n",
       "39   0.002802          decid\n",
       "56   0.002785        everyth\n",
       "138  0.002782           read\n",
       "146  0.002755           said\n",
       "83   0.002749           high\n",
       "7    0.002746          anoth\n",
       "65   0.002731           fine\n",
       "55   0.002710        everyon\n",
       "2    0.002709         actual\n",
       "145  0.002703          right\n",
       "196  0.002693          worth\n",
       "87   0.002692           hour\n",
       "159  0.002675         someth\n",
       "126  0.002553            old\n",
       "179  0.002543           told\n",
       "111  0.002537            may\n",
       "119  0.002530          negat\n",
       "0    0.002505        absolut\n",
       "186  0.002457            way\n",
       "63   0.002420      feel like\n",
       "\n",
       "[199 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sort_values(by = 'imp' , ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'type' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-bd1c137c451f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnormalize_corpus_outside\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"i loved this birth control. i used to have so much bloating with my previous pills and this one is honestly the best. my sex drive is better and I don't have side effects of wieght gain.\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;31m# pre process, I do the count vectorizer, model.predict (on the verized form)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'type' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "normalize_corpus_outside = normalize_corpus(list[\"i loved this birth control. i used to have so much bloating with my previous pills and this one is honestly the best. my sex drive is better and I don't have side effects of wieght gain.\"] ) # pre process, I do the count vectorizer, model.predict (on the verized form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "CountVectorizer - Vocabulary wasn't fitted.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-466123d34f80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm_corpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1081\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1083\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1084\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[1;34m\"\"\"Check if vocabulary is empty or missing (not fit-ed)\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"%(name)s - Vocabulary wasn't fitted.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vocabulary_'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 951\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: CountVectorizer - Vocabulary wasn't fitted."
     ]
    }
   ],
   "source": [
    "x = vectorizer.transform(norm_corpus_outside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RF.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do a distinct norm corpus for each condition\n",
    "#the first part where you split out birth control\n",
    "#should be the base of your loop\n",
    "#for loop looks like this\n",
    "#your for loop should look like\n",
    "#models = {}\n",
    "#for condition in list_of_conditions:\n",
    " #norm_corpus = norm(df[df['condition']==condition])\n",
    " #X = vecorizor(norm_corpus)\n",
    " #Y = df[df['condition']==condition]['rating']\n",
    " #model = train_model(X,Y)\n",
    " #models[condition]=model\n",
    "    \n",
    "#accuracies = {}\n",
    "#accuracies[condition] = rsquared(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Predict & Evaluate Extreme Gradient Boosted Model with tuned hyperparameters\n",
    "\n",
    "##### Why Use XGBoost?\n",
    "The two reasons to use XGBoost are also the two goals of the project:\n",
    "\n",
    "+ Execution Speed.\n",
    "+ Model Performance.\n",
    "XGBoost dominates structured or tabular datasets on classification and regression predictive modeling problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading https://files.pythonhosted.org/packages/51/c1/198915b13e98b62a98f48309c41012638464651da755d941f4abe384c012/xgboost-0.82-py2.py3-none-win_amd64.whl (7.7MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\nazanin\\anaconda3\\lib\\site-packages (from xgboost) (1.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\nazanin\\anaconda3\\lib\\site-packages (from xgboost) (1.15.4)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-0.82\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "mingw_path = r'C:\\mingw-w64\\mingw64\\bin'\n",
    "os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=42, silent=True,\n",
       "       subsample=1)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_xgb = xgb.XGBClassifier(seed=42)\n",
    "clf_xgb.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.  8. 10. ... 10. 10. 10.]\n"
     ]
    }
   ],
   "source": [
    "clf_xgb_predictions = clf_xgb.predict(xtest)\n",
    "print(clf_xgb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-65238928b499>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'n_estimators'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'max_depth'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclf_RF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'exact'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mclf_RF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf_RF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    720\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    709\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 711\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 920\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    921\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[0;32m    711\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 713\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"objective\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[0;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[1;32m-> 1110\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Get the best hyperparameter values\n",
    "param_grid = {'n_estimators': [100, 200, 300],'max_depth': [5, 10, 15],'learning_rate': [0.3, 0.5]}\n",
    "clf_xgb = GridSearchCV(xgb.XGBClassifier(tree_method='exact', seed=42), param_grid, cv=5, scoring='accuracy')\n",
    "clf_xgb.fit(xtrain, ytrain)\n",
    "print(clf_xgb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = clf_xgb.cv_results_\n",
    "for param, score_mean, score_sd in zip(results['params'], results['mean_test_score'], results['std_test_score']):\n",
    "    print(param, round(score_mean, 4), round(score_sd, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_xgb = xgb.XGBClassifier(seed=42, max_depth=10, learning_rate=0.3, n_estimators=100)\n",
    "clf_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "clf_xgb_predictions = clf_xgb.predict(xtest)\n",
    "print(clf_xgb_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
