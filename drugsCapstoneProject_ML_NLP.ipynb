{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning -- NLP \n",
    "\n",
    "For the final phase of this project, NLP Machine Learning is performed. \n",
    "Due to the fact that the dataset provides patient reviews on specific drugs associated with related conditions and a 10-star patient rating reflecting overall patient satisfaction of the drug chosen for that specific condition, we choose Natural Language Processing Machine Learning techniques in predicting the rating for the reviews of the drugs for the first four most reviewed conditions of Birth Control, Depression, Pain and Anxiety. In the following cells, we import the needed libraries and then get our data set ready for the machine learning techniques we will choose. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nazanin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import sklearn\n",
    "import scipy as sp\n",
    "import nltk\n",
    "import re \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.display.max_colwidth = 200\n",
    "%matplotlib inline\n",
    "#import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "#from matplotlib import pyplot\n",
    "import pylab\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from scipy.stats import norm \n",
    "from scipy import stats \n",
    "from statsmodels.stats.weightstats import ztest\n",
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Nazanin\\Downloads\\drugsCom_raw\\drugsComTest_raw.csv', sep=\"\\t\")\n",
    "df1 = pd.read_csv(r'C:\\Users\\Nazanin\\Downloads\\drugsCom_raw\\drugsComTrain_raw.csv', sep=\"\\t\")\n",
    "#display(df, df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'drugName',\n",
       " 'condition',\n",
       " 'review',\n",
       " 'rating',\n",
       " 'date',\n",
       " 'usefulCount']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.append(df1)\n",
    "list(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df2 = df2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0         drugName                     condition  \\\n",
      "0      163740      Mirtazapine                    Depression   \n",
      "1      206473       Mesalamine  Crohn's Disease, Maintenance   \n",
      "2      159672          Bactrim       Urinary Tract Infection   \n",
      "3       39293         Contrave                   Weight Loss   \n",
      "4       97768  Cyclafem 1 / 35                 Birth Control   \n",
      "\n",
      "                                                                                                                                                                                                    review  \\\n",
      "0  \"I&#039;ve tried a few antidepressants over the years (citalopram, fluoxetine, amitriptyline), but none of those helped with my depression, insomnia &amp; anxiety. My doctor suggested and changed ...   \n",
      "1  \"My son has Crohn&#039;s disease and has done very well on the Asacol.  He has no complaints and shows no side effects.  He has taken as many as nine tablets per day at one time.  I&#039;ve been v...   \n",
      "2                                                                                                                                                                            \"Quick reduction of symptoms\"   \n",
      "3  \"Contrave combines drugs that were used for alcohol, smoking, and opioid cessation. People lose weight on it because it also helps control over-eating. I have no doubt that most obesity is caused ...   \n",
      "4  \"I have been on this birth control for one cycle. After reading some of the reviews on this type and similar birth controls I was a bit apprehensive to start. Im giving this birth control a 9 out ...   \n",
      "\n",
      "   rating                date  usefulCount  \n",
      "0    10.0   February 28, 2012           22  \n",
      "1     8.0        May 17, 2009           17  \n",
      "2     9.0  September 29, 2017            3  \n",
      "3     9.0       March 5, 2017           35  \n",
      "4     9.0    October 22, 2015            4  \n"
     ]
    }
   ],
   "source": [
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Levonorgestrel                        4930\n",
       "Etonogestrel                          4421\n",
       "Ethinyl estradiol / norethindrone     3753\n",
       "Nexplanon                             2892\n",
       "Ethinyl estradiol / norgestimate      2790\n",
       "Ethinyl estradiol / levonorgestrel    2503\n",
       "Phentermine                           2085\n",
       "Sertraline                            1868\n",
       "Escitalopram                          1747\n",
       "Mirena                                1673\n",
       "Name: drugName, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.drugName.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Birth Control      38436\n",
       "Depression         12164\n",
       "Pain                8245\n",
       "Anxiety             7812\n",
       "Acne                7435\n",
       "Bipolar Disorde     5604\n",
       "Insomnia            4904\n",
       "Weight Loss         4857\n",
       "Obesity             4757\n",
       "ADHD                4509\n",
       "Name: condition, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.condition.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by checking the Birth Control reviews and get a sense of the review feature for this condition in our data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4     \"I have been on this birth control for one cycle. After reading some of the reviews on this type and similar birth controls I was a bit apprehensive to start. Im giving this birth control a 9 out ...\n",
      "6     \"I&#039;ve had the copper coil for about 3 months now. I was really excited at the thought of not taking hormones. I&#039;m good with pain however I nearly fainted with insertion, couldn&#039;t be...\n",
      "9     \"I was on this pill for almost two years. It does work as far as not getting pregnant however my experience at first was it didn&#039;t make a huge difference then 6 or 7 months into it my sex dri...\n",
      "30    \"I absolutely love this product and recommend to everyone. I know everyone&#039;s body is different, so it is not for everyone, but it is not the medicines fault. I have NO negative symptoms since...\n",
      "37    \"I was on this for 5 years (and birth control pills for about 12 years), and would have told you how fabulous it was.  &lt;List all the benefits everyone else has listed, here.&gt;  Then a friend ...\n",
      "Name: review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df2[df2['condition'] == 'Birth Control'].review.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48                                                                                                                                                                                              \"Works for me\"\n",
      "63     \"I was prescribed this for onset of anxiety and possible hormonal mood swings. I was not told by my doctor how it would make me feel or how hard coming off of it would be. I took one 37.5 mg capsu...\n",
      "83     \"I did not like this medication. For anxiety, I have also tried Hydroxyzine (Atarax). I guess this is just my personal body chemistry but I actually prefer Atarax to this unlike most people. This ...\n",
      "133    \"I&#039;m a 32 year old male and I&#039;ve been taking buspar for about 10 months. At first it did nothing but make my anxiety worse. I would wake up with to full blown panic attacks and have the ...\n",
      "208    \"Klonopin is a very effective medicine for people such as myself that suffer from debilitating panic disorder and/or PTSD.  This medicine saved me from becoming institutionalized after returning h...\n",
      "Name: review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df2[df2['condition'] == 'Anxiety'].review.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before delving into the machine learning techniques, let's take a look at the bar plot of the review counts of ratings, rating from 1 to 10, and by the look of the following plot, we can see that the highly rated reviews as well as the low lated reviews have the highest review count compared to other rating counts for review. This shows that patients are motivated to rate a drug when either they like or dislike it very much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "df2.groupby('rating').review.count().plot.bar(ylim=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215063"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.review.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38436"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[df2['condition'] == 'Birth Control'].review.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ration of the birth control review to the reviews of the entire data set: 0.1787197239878547\n"
     ]
    }
   ],
   "source": [
    "print(\"The ration of the birth control review to the reviews of the entire data set: \" + str(df2[df2['condition'] == 'Birth Control'].review.count()/df2.review.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review feature of the Data Set -- NLP \n",
    "+ On the Machine Learning phase, we start looking at the text documents of the review column. In order to do that, we need to do some text pre-processing. We process and tokenize corpus of reviews to build features for predictive models. Also, train machine learning models to predict drug rating based on reviews and relevant metadata.\n",
    "+ In order to do that, we need to pre-process our text data by removing tags, i.e. HTML tags, as well as removing accented characters, é to e., special characters, which mainly add to the extra noise than giving benefits. We also need to stem the words as well as lemmatize in cases (stemming is cutting the word and keeping the root of the word and lemmatization is that the remaining root is grammatically a correct form but stem of the word is not). Another step in text pre-processing is to remove the Stop words. Stop words are the words with little and insignificance meaning in the corpus. Words such as a, an, the are examples of Stop words in English. Last but not least, we remove extra whitespaces, text lower casing as well as spelling corrections, grammatical error corrections, removing repeated characters to name a few. We then tokenize the corpus to build features for predictive models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text pre-processing\n",
    "\n",
    "+ Removing tags: Removing unnecessary HTML tags, etc. which do not add much value when analyzing text (The BeautifulSoup library does an excellent job in providing necessary functions for this.).\n",
    "\n",
    "+ Removing accented characters: Removing  accented characters\\letters in the text corpus and convert these characters and standardized into ASCII characters, an example is to convert é to e. \n",
    "\n",
    "+ Expanding contractions: Converting contraction to its expanded, original form often helps with text standardization, example of which would be, \"do not\" to \"don’t\" and \"I would\" to \"I’d\".\n",
    "\n",
    "+ Removing special characters: Removing special characters and symbols often add to the extra noise in unstructured text. More than often, simple regular expressions (regexes) can be used to achieve this.\n",
    "\n",
    "+ Stemming and lemmatization: The reverse process of inflection is called stemming, which is basically obtaining the base form of a word. Lemmatization is very similar to stemming, the difference being that the root word from lemmatization is always a lexicographically correct word but the root stem may not be so.\n",
    "\n",
    "+ Removing stopwords: Words with little or no significance in text corpus are known as stopwords, example of which are:  a, an, the. We can use a standard English language stopwords list from nltk and also add our own domain specific stopwords as needed.\n",
    "\n",
    "+ Other text cleaning to do would be tokenization, removing extra whitespaces, text lower casing as well as spelling corrections, grammatical error corrections, removing repeated characters to name a few. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove special characters, extra whitespaces, digits, stopwords and lower casing the text corpus\n",
    "#result = re.sub(pattern, repl, string, count=0, flags=0);\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)   ###plscheck this \n",
    "wpt = nltk.WordPunctTokenizer()  # $4.99 \n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words.append('</span> users found this comment helpful.') \n",
    "stop_words.append('&#039')\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A) \n",
    "    #re.I A means treats . as whatever it is. Performs case-insensitive matching. #issues with FLAGS!!!!!\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words] \n",
    "    filtered_tokens = [stemmer.stem(word) for word in filtered_tokens]  \n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "# doc is the list of strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first attempt to analyze the rating of the reviews of the birth control drugs since birth control was the highest count for the condition in the data set with almost one fifth of the entire data set. Moreover, the number of reviews for birth control was the highest compared to the other conditions with the same ratio of 18%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_corpus = normalize_corpus(df2[df2['condition'] == 'Birth Control'].review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['birth control one cycl read review type similar birth control bit apprehens start im give birth control long enough far love birth control side effect minim like im even birth control experienc mild headach nausea ive feel great got period cue third day inact pill idea come zero pms period light bare cramp unprotect sex first month obvious didnt get pregnant im pleas high recommend',\n",
       "       'ive copper coil month realli excit thought take hormon im good pain howev near faint insert couldnt beliv pain doctor say pain well month period last day im pain day random twang especi left side im consid whether want put intens pain heavi period id recommend somebodi doesnt alreadi heavi pain period right isnt',\n",
       "       'pill almost two year work far get pregnant howev experi first didnt make huge differ month sex drive went along dri moodi increas drastic would cri one second get angri husband anyth everyth skin gotten lot wors broke place never last week yaz',\n",
       "       ...,\n",
       "       'experi pain insert expect sinc ive never children lot bloat cramp breast tender first coupl week use pamprin heat pad belli help week insert came brown sludg last ten day first month studi miseri period stop breez occasion light cramp spot drawback person year old perimenopaus period make imposs track cycl overal delight',\n",
       "       'nexplanon sinc dec got first period end januari last month half march didnt bleed close three week start bleed march th bleed everi sinc gain lbs far sinc get birth control although weight gain isnt deal breaker bleed tri patient see bodi adjust implant three month far finger cross cycl go away awhil',\n",
       "       'would second month junel ive birth control year chang due spot increas mood swing previous birth control sinc switch shorter period day gain major weight increas appetit switch regular exercis routin still manag drop extra lbs'],\n",
       "      dtype='<U2960')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get more familiar with our birth control reviews, let us see what words are mostly used in that corpus. For this reason, we define a function to count the top \"n\" words in the corpus for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=None):\n",
    "    \"\"\"\n",
    "    List the top n words in a vocabulary according to occurrence in a text corpus.\n",
    "    \n",
    "    get_top_n_words([\"I love Python\", \"Python is a language programming\", \"Hello world\", \"I love the world\"]) -> \n",
    "    [('python', 2),\n",
    "     ('world', 2),\n",
    "     ('love', 2),\n",
    "     ('hello', 1),\n",
    "     ('is', 1),\n",
    "     ('programming', 1),\n",
    "     ('the', 1),\n",
    "     ('language', 1)]\n",
    "    \"\"\"\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('period', 37273),\n",
       " ('month', 36515),\n",
       " ('pill', 26390),\n",
       " ('get', 23071),\n",
       " ('ive', 21923),\n",
       " ('day', 21659),\n",
       " ('im', 20430),\n",
       " ('year', 19498),\n",
       " ('control', 18934),\n",
       " ('week', 18639),\n",
       " ('birth', 18338),\n",
       " ('take', 18146),\n",
       " ('first', 17184),\n",
       " ('cramp', 16278),\n",
       " ('gain', 15381),\n",
       " ('weight', 15367),\n",
       " ('start', 15324),\n",
       " ('effect', 14001),\n",
       " ('got', 13875),\n",
       " ('time', 13647),\n",
       " ('pain', 13167),\n",
       " ('like', 13144),\n",
       " ('side', 12075),\n",
       " ('bleed', 11932),\n",
       " ('would', 11297),\n",
       " ('insert', 11113),\n",
       " ('feel', 10858),\n",
       " ('sinc', 10519),\n",
       " ('mood', 10392),\n",
       " ('acn', 10089),\n",
       " ('never', 9849),\n",
       " ('spot', 9770),\n",
       " ('sex', 9433),\n",
       " ('bad', 9376),\n",
       " ('go', 9145),\n",
       " ('use', 9100),\n",
       " ('also', 9021),\n",
       " ('one', 8951),\n",
       " ('last', 8476),\n",
       " ('dont', 8321)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n_words(normalize_corpus(df2[df2['condition'] == 'Birth Control'].review), n=40)  #None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, words such as period, month, pill, etc. are among the highest count in the birth control corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "period 37273\n",
      "month 36515\n",
      "pill 26390\n",
      "get 23071\n",
      "ive 21923\n",
      "day 21659\n",
      "im 20430\n",
      "year 19498\n",
      "control 18934\n",
      "week 18639\n",
      "birth 18338\n",
      "take 18146\n",
      "first 17184\n",
      "cramp 16278\n",
      "gain 15381\n",
      "weight 15367\n",
      "start 15324\n",
      "effect 14001\n",
      "got 13875\n",
      "time 13647\n"
     ]
    }
   ],
   "source": [
    "common_words_BC = get_top_n_words(normalize_corpus(df2[df2['condition'] == 'Birth Control'].review), 20)\n",
    "for word, freq in common_words_BC:\n",
    "    print(word, freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizing CountVectorizer, we group the words in the corpus one by one or 2 by 2. This is called \"n-gram\". We start by 2-gram and we implement machine learning techniques to check the performance of the 2-gram and then compare it with 1-gram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed text vector is \n",
      " [[5 0 1 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 1]\n",
      " [2 0 0 ... 0 0 0]]\n",
      "Words for each feature:\n",
      "['birth control', 'feel like', 'first month', 'gain weight', 'mood swing', 'sex drive', 'side effect', 'take pill', 'weight gain']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(2,2), min_df=0.05, max_df=1.0, max_features=500)\n",
    "\n",
    "# call `fit` to build the vocabulary\n",
    "vectorizer.fit(norm_corpus)\n",
    "\n",
    "# call `transform` to convert text to a bag of words\n",
    "x = vectorizer.transform(norm_corpus)\n",
    "\n",
    "# CountVectorizer uses a sparse array to save memory, but it's easier in this assignment to \n",
    "# convert back to a \"normal\" numpy array\n",
    "x = x.toarray()  # this is for visualization purposes \n",
    "\n",
    "print()\n",
    "print(\"Transformed text vector is \\n\", x)\n",
    "\n",
    "# `get_feature_names` tracks which word is associated with each column of the transformed x\n",
    "print\n",
    "print(\"Words for each feature:\")\n",
    "print(vectorizer.get_feature_names())  # visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38436, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the corpus ready for NLP Machine Learning techniques, we are now about to choose the proper techniques to be able to train the train set and be able to predict on the test set. Naive Bayes, Random Forest are among the most popular feature predicting models. \n",
    "The criteria on how to choose the proper machine learning technique is the performance and efficiency of the technique. In this project, we choose Naïve Bayes, Random Forest and Extreme Gradient Boosting techniques and we implement them on the rankings of the reviews of the four mostly reviewed and populated conditions in the data set, namely, Birth Control, Depression, Pain and Anxiety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df2[df2['condition'] == 'Birth Control'].rating  # df2['rating']\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y)  #(default=0.25)\n",
    "clf = MultinomialNB().fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = clf.score(xtrain, ytrain)\n",
    "test_accuracy = clf.score(xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MN Accuracy: 20.79%\n",
      "Accuracy on training data: 0.21\n",
      "Accuracy on test data:     0.21\n"
     ]
    }
   ],
   "source": [
    "print(\"MN Accuracy: %0.2f%%\" % (100 * clf.score(xtest, ytest)))\n",
    "print(\"Accuracy on training data: %0.2f\" % (training_accuracy))\n",
    "print(\"Accuracy on test data:     %0.2f\" % (test_accuracy))   #30% is precicely predicting the ratings for df2_min = 0.1\n",
    "# do a good and bad as a binary! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes prediction turned out at a disappointing rate of only 21%! Let's switch to other models and see how they perform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nazanin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    " #regressor target variables rating is \n",
    "# Train the model with Random Foprest Classifier     \n",
    "clf_RF = RandomForestClassifier()  \n",
    "#n_jobs=2 number of cores the computer uses, n_estimators = 500 (number of trees)  startt 50 \n",
    "clf_RF.fit(xtrain, ytrain)  \n",
    "# predict and evaluate performance\n",
    "clf_RF_predictions = clf_RF.predict(xtest)\n",
    "#meu.display_model_performance_metrics(true_labels=ytest, predicted_labels=clf_RF_predictions , classes= range(1,11))\n",
    "#hash maps a value to another value and it's more efficient in computation\n",
    "# normaization removing bias that could exists in a data set \n",
    "# correcting for the size of the corpus. one word review and the other is 200000 words \n",
    "# and contains the word good and we should \n",
    "# normaliztion in NLP is to use word frequecy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20584546 0.06917504 0.0826262  0.08909746 0.15301874 0.08619107\n",
      " 0.10124585 0.12122744 0.09157273]\n"
     ]
    }
   ],
   "source": [
    "print(clf_RF.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "print(clf_RF.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': None, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "#gridsearch and hyoerparameter tuning (on the TRAIN set) \n",
    "param_grid = {'n_estimators': [100, 200, 300, 500], 'max_features': ['auto', None, 'log2']}\n",
    "clf_RF = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "clf_RF.fit(xtrain, ytrain)\n",
    "print(clf_RF.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 'auto', 'n_estimators': 100} 0.2268 0.0033\n",
      "{'max_features': 'auto', 'n_estimators': 200} 0.2258 0.003\n",
      "{'max_features': 'auto', 'n_estimators': 300} 0.2267 0.0026\n",
      "{'max_features': 'auto', 'n_estimators': 500} 0.2268 0.003\n",
      "{'max_features': None, 'n_estimators': 100} 0.2269 0.0036\n",
      "{'max_features': None, 'n_estimators': 200} 0.2259 0.0028\n",
      "{'max_features': None, 'n_estimators': 300} 0.2266 0.0027\n",
      "{'max_features': None, 'n_estimators': 500} 0.2268 0.0028\n",
      "{'max_features': 'log2', 'n_estimators': 100} 0.2268 0.0033\n",
      "{'max_features': 'log2', 'n_estimators': 200} 0.2258 0.003\n",
      "{'max_features': 'log2', 'n_estimators': 300} 0.2267 0.0026\n",
      "{'max_features': 'log2', 'n_estimators': 500} 0.2268 0.003\n"
     ]
    }
   ],
   "source": [
    "results_22gram = clf_RF.cv_results_\n",
    "for param, score_mean, score_sd in zip(results['params'], results['mean_test_score'], results['std_test_score']):\n",
    "    print(param, round(score_mean, 4), round(score_sd, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best hyperparameter performance occurs at: {'max_features': None, 'n_estimators': 100} 0.2269 0.0036"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RF = RandomForestClassifier(n_estimators=100, max_features=None, random_state=42)\n",
    "clf_RF.fit(xtrain, ytrain)\n",
    "clf_RF_predictions = clf_RF.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 296   20   16    7    9    6   12   36   70  990]\n",
      " [ 145   23    7    6    8    4   11   30   45  464]\n",
      " [ 108   21   38    3    6    3    6   27   44  426]\n",
      " [ 100   11    3   14   11    6   12   29   30  297]\n",
      " [ 128   17    5    7   26    4    4   38   55  468]\n",
      " [  73   17    4    4    4   27    7   22   43  280]\n",
      " [  79    7    6    6    9    2   24   33   61  385]\n",
      " [ 144   17    6    7   14    6   17  123   63  651]\n",
      " [ 159   14   20    3   10    6   15   66  194  937]\n",
      " [ 169   29   10    3   10    7   17   75  125 1447]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.21      0.20      0.21      1462\n",
      "         2.0       0.13      0.03      0.05       743\n",
      "         3.0       0.33      0.06      0.10       682\n",
      "         4.0       0.23      0.03      0.05       513\n",
      "         5.0       0.24      0.03      0.06       752\n",
      "         6.0       0.38      0.06      0.10       481\n",
      "         7.0       0.19      0.04      0.07       612\n",
      "         8.0       0.26      0.12      0.16      1048\n",
      "         9.0       0.27      0.14      0.18      1424\n",
      "        10.0       0.23      0.76      0.35      1892\n",
      "\n",
      "   micro avg       0.23      0.23      0.23      9609\n",
      "   macro avg       0.25      0.15      0.13      9609\n",
      "weighted avg       0.24      0.23      0.17      9609\n",
      "\n",
      "0.23020085336663546\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(ytest,clf_RF_predictions))  \n",
    "print(classification_report(ytest,clf_RF_predictions))  \n",
    "print(accuracy_score(ytest, clf_RF_predictions)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, even Random Forest couldn't do much better than NB method so the next thing to do is to switch the (2,2) gram to (1,2) gram and see the performance of our models accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2560099906337808"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "clf_RF.score(x, y, sample_weight=None)  # on the entire data here so obviously it goes up by a few percentages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are performing ngram(1,2): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed text vector is \n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 1]\n",
      " ...\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 1]]\n",
      "Words for each feature:\n",
      "['absolut', 'acn', 'actual', 'ago', 'almost', 'also', 'alway', 'anoth', 'anxieti', 'anyth', 'around', 'away', 'back', 'bad', 'bc', 'best', 'better', 'birth', 'birth control', 'bit', 'bleed', 'bloat', 'bodi', 'break', 'breast', 'cant', 'caus', 'chang', 'clear', 'come', 'complet', 'constant', 'control', 'could', 'coupl', 'cramp', 'crazi', 'cri', 'day', 'decid', 'definit', 'depress', 'didnt', 'differ', 'doctor', 'dont', 'drive', 'due', 'eat', 'effect', 'emot', 'end', 'even', 'ever', 'everi', 'everyon', 'everyth', 'experi', 'experienc', 'extrem', 'face', 'far', 'feel', 'feel like', 'felt', 'fine', 'first', 'first month', 'gain', 'gain weight', 'get', 'give', 'go', 'good', 'got', 'gotten', 'great', 'half', 'happi', 'havent', 'headach', 'heavi', 'help', 'high', 'hope', 'hormon', 'horribl', 'hour', 'howev', 'hurt', 'im', 'implanon', 'implant', 'increas', 'insert', 'issu', 'iud', 'ive', 'know', 'last', 'lbs', 'life', 'light', 'like', 'littl', 'long', 'lost', 'lot', 'love', 'made', 'make', 'may', 'mirena', 'month', 'mood', 'mood swing', 'moodi', 'much', 'nausea', 'negat', 'never', 'nexplanon', 'next', 'normal', 'noth', 'notic', 'old', 'one', 'overal', 'pain', 'period', 'pill', 'pound', 'pregnanc', 'pregnant', 'pretti', 'problem', 'put', 'read', 'realli', 'reason', 'recommend', 'regular', 'remov', 'review', 'right', 'said', 'say', 'second', 'see', 'seem', 'sever', 'sex', 'sex drive', 'shot', 'side', 'side effect', 'sinc', 'skin', 'someth', 'spot', 'start', 'still', 'stop', 'sure', 'swing', 'switch', 'symptom', 'take', 'take pill', 'taken', 'terribl', 'that', 'thing', 'think', 'though', 'thought', 'three', 'time', 'told', 'took', 'tri', 'two', 'use', 'want', 'wasnt', 'way', 'week', 'weight', 'weight gain', 'well', 'went', 'work', 'worri', 'wors', 'worst', 'worth', 'would', 'year']\n"
     ]
    }
   ],
   "source": [
    "# So now, let's change the ngram to (1,2)\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2), min_df=0.05, max_df=1.0, max_features=500)\n",
    "\n",
    "# call `fit` to build the vocabulary\n",
    "vectorizer.fit(norm_corpus)\n",
    "\n",
    "# call `transform` to convert text to a bag of words\n",
    "x = vectorizer.transform(norm_corpus)\n",
    "\n",
    "# CountVectorizer uses a sparse array to save memory, but it's easier in this assignment to \n",
    "# convert back to a \"normal\" numpy array\n",
    "x = x.toarray()  # this is for visualization purposes \n",
    "\n",
    "print()\n",
    "print(\"Transformed text vector is \\n\", x)\n",
    "\n",
    "# `get_feature_names` tracks which word is associated with each column of the transformed x\n",
    "print\n",
    "print(\"Words for each feature:\")\n",
    "print(vectorizer.get_feature_names())  # visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38436, 199)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default division percentage between train and test set is 25%, meaning that 25% of the data is held for test and 75% is being trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df2[df2['condition'] == 'Birth Control'].rating  # df2['rating']\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y)  #(default=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nazanin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    " #regressor target variables rating is \n",
    "# Train the model with Random Foprest Classifier     \n",
    "clf_RF = RandomForestClassifier()  \n",
    "#n_jobs=2 number of cores the computer uses, n_estimators = 500 (number of trees)  startt 50 \n",
    "clf_RF.fit(xtrain, ytrain)  \n",
    "# predict and evaluate performance\n",
    "clf_RF_predictions = clf_RF.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00269789 0.00767639 0.00260132 0.00460638 0.00539263 0.00721835\n",
      " 0.00396282 0.00277185 0.0030831  0.0028311  0.00259909 0.0034435\n",
      " 0.00613394 0.00701839 0.00310071 0.00410683 0.00340357 0.00829328\n",
      " 0.00800549 0.00283173 0.0091929  0.0033075  0.00403647 0.00286612\n",
      " 0.00452014 0.0037435  0.00425346 0.00512302 0.00330891 0.00303567\n",
      " 0.00266628 0.00461462 0.00853098 0.00352482 0.00290925 0.00914174\n",
      " 0.00306992 0.0034349  0.01083917 0.00292782 0.00284916 0.00775866\n",
      " 0.00601178 0.00459086 0.0055754  0.00694947 0.00451654 0.00275842\n",
      " 0.00289245 0.00698149 0.00382701 0.00260396 0.00467298 0.00447278\n",
      " 0.00629426 0.00245419 0.00289067 0.0050974  0.00511957 0.00460053\n",
      " 0.00289372 0.00628534 0.00769323 0.00272798 0.00442675 0.00282862\n",
      " 0.00988772 0.00339204 0.00832815 0.00325487 0.01228586 0.00347653\n",
      " 0.00707582 0.00562251 0.00875473 0.00308077 0.00573606 0.00273681\n",
      " 0.00321603 0.00458847 0.00465476 0.00507328 0.0045402  0.0027949\n",
      " 0.00387815 0.00364149 0.00585481 0.00266023 0.00499218 0.00299733\n",
      " 0.01194735 0.00343779 0.00365141 0.00269339 0.00746064 0.00326727\n",
      " 0.0031011  0.01182335 0.00349849 0.0066151  0.00415947 0.00398666\n",
      " 0.00504054 0.00889925 0.00518937 0.00329912 0.00318629 0.00486613\n",
      " 0.01434073 0.00506694 0.00477886 0.00250549 0.00389878 0.01468978\n",
      " 0.00695183 0.0054463  0.00366329 0.0047908  0.00365725 0.00238381\n",
      " 0.00702    0.00387561 0.00285331 0.00460426 0.00373103 0.00556245\n",
      " 0.00278401 0.00665368 0.00334601 0.00820888 0.01578582 0.01163128\n",
      " 0.00420371 0.00397024 0.00565253 0.00339107 0.0050547  0.00466546\n",
      " 0.00309641 0.00620623 0.00275461 0.00592398 0.00297603 0.00564827\n",
      " 0.00399147 0.00280253 0.00270302 0.00409322 0.00353555 0.00314843\n",
      " 0.00330286 0.00407156 0.00663449 0.00393344 0.00307678 0.00608198\n",
      " 0.0047716  0.00724321 0.00409323 0.00265521 0.00714611 0.00983894\n",
      " 0.00553428 0.00729353 0.00362294 0.00487769 0.00577323 0.00273826\n",
      " 0.0103386  0.00374751 0.00294197 0.00362631 0.00262051 0.00563511\n",
      " 0.00493413 0.00332211 0.00334137 0.00294844 0.00893552 0.00233374\n",
      " 0.0043498  0.00559923 0.00598276 0.00697221 0.00443345 0.00308968\n",
      " 0.00254533 0.01084395 0.00852216 0.00449662 0.00397473 0.00537634\n",
      " 0.00628835 0.00288012 0.00346393 0.00619994 0.00267089 0.00795341\n",
      " 0.00964957]\n"
     ]
    }
   ],
   "source": [
    "print(clf_RF.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1326   24   27    6    8    3    8   13   21   39]\n",
      " [  94  549   18    2    4    2    2    8   10   24]\n",
      " [  69   20  510    2   12    0    4    7   27   18]\n",
      " [  57    8   12  393   11    8    6   20   20   18]\n",
      " [  79   10   19    3  553    0   11   20   14   24]\n",
      " [  27    6    8   12   12  355    0   15   21   21]\n",
      " [  38    8    6    8   14    2  474   15   17   37]\n",
      " [  33   12   17   11    9    4    7  791   64   67]\n",
      " [  43   14   10    9   11    4   26   48 1144  131]\n",
      " [  69   16   10    7   21   12    6   48  133 1593]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.72      0.90      0.80      1475\n",
      "         2.0       0.82      0.77      0.80       713\n",
      "         3.0       0.80      0.76      0.78       669\n",
      "         4.0       0.87      0.71      0.78       553\n",
      "         5.0       0.84      0.75      0.80       733\n",
      "         6.0       0.91      0.74      0.82       477\n",
      "         7.0       0.87      0.77      0.82       619\n",
      "         8.0       0.80      0.78      0.79      1015\n",
      "         9.0       0.78      0.79      0.79      1440\n",
      "        10.0       0.81      0.83      0.82      1915\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      9609\n",
      "   macro avg       0.82      0.78      0.80      9609\n",
      "weighted avg       0.81      0.80      0.80      9609\n",
      "\n",
      "0.8000832552815069\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(ytest,clf_RF_predictions))  \n",
    "print(classification_report(ytest,clf_RF_predictions))  \n",
    "print(accuracy_score(ytest, clf_RF_predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "print(clf_RF.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the feature importance table corresponding to feature names:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imp</th>\n",
       "      <th>vars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002698</td>\n",
       "      <td>absolut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007676</td>\n",
       "      <td>acn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002601</td>\n",
       "      <td>actual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004606</td>\n",
       "      <td>ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005393</td>\n",
       "      <td>almost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.007218</td>\n",
       "      <td>also</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.003963</td>\n",
       "      <td>alway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.002772</td>\n",
       "      <td>anoth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.003083</td>\n",
       "      <td>anxieti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.002831</td>\n",
       "      <td>anyth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.002599</td>\n",
       "      <td>around</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.003443</td>\n",
       "      <td>away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.006134</td>\n",
       "      <td>back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.007018</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.003101</td>\n",
       "      <td>bc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.004107</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.003404</td>\n",
       "      <td>better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.008293</td>\n",
       "      <td>birth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.008005</td>\n",
       "      <td>birth control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.002832</td>\n",
       "      <td>bit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.009193</td>\n",
       "      <td>bleed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.003308</td>\n",
       "      <td>bloat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.004036</td>\n",
       "      <td>bodi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.002866</td>\n",
       "      <td>break</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.004520</td>\n",
       "      <td>breast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.003743</td>\n",
       "      <td>cant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.004253</td>\n",
       "      <td>caus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.005123</td>\n",
       "      <td>chang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.003309</td>\n",
       "      <td>clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.003036</td>\n",
       "      <td>come</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0.003748</td>\n",
       "      <td>take pill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0.002942</td>\n",
       "      <td>taken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.003626</td>\n",
       "      <td>terribl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0.002621</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.005635</td>\n",
       "      <td>thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0.004934</td>\n",
       "      <td>think</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.003322</td>\n",
       "      <td>though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.003341</td>\n",
       "      <td>thought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.002948</td>\n",
       "      <td>three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.008936</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.002334</td>\n",
       "      <td>told</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.004350</td>\n",
       "      <td>took</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.005599</td>\n",
       "      <td>tri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.005983</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0.006972</td>\n",
       "      <td>use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.004433</td>\n",
       "      <td>want</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.003090</td>\n",
       "      <td>wasnt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.002545</td>\n",
       "      <td>way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.010844</td>\n",
       "      <td>week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.008522</td>\n",
       "      <td>weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.004497</td>\n",
       "      <td>weight gain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.003975</td>\n",
       "      <td>well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.005376</td>\n",
       "      <td>went</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.006288</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.002880</td>\n",
       "      <td>worri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0.003464</td>\n",
       "      <td>wors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.006200</td>\n",
       "      <td>worst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.002671</td>\n",
       "      <td>worth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.007953</td>\n",
       "      <td>would</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.009650</td>\n",
       "      <td>year</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          imp           vars\n",
       "0    0.002698        absolut\n",
       "1    0.007676            acn\n",
       "2    0.002601         actual\n",
       "3    0.004606            ago\n",
       "4    0.005393         almost\n",
       "5    0.007218           also\n",
       "6    0.003963          alway\n",
       "7    0.002772          anoth\n",
       "8    0.003083        anxieti\n",
       "9    0.002831          anyth\n",
       "10   0.002599         around\n",
       "11   0.003443           away\n",
       "12   0.006134           back\n",
       "13   0.007018            bad\n",
       "14   0.003101             bc\n",
       "15   0.004107           best\n",
       "16   0.003404         better\n",
       "17   0.008293          birth\n",
       "18   0.008005  birth control\n",
       "19   0.002832            bit\n",
       "20   0.009193          bleed\n",
       "21   0.003308          bloat\n",
       "22   0.004036           bodi\n",
       "23   0.002866          break\n",
       "24   0.004520         breast\n",
       "25   0.003743           cant\n",
       "26   0.004253           caus\n",
       "27   0.005123          chang\n",
       "28   0.003309          clear\n",
       "29   0.003036           come\n",
       "..        ...            ...\n",
       "169  0.003748      take pill\n",
       "170  0.002942          taken\n",
       "171  0.003626        terribl\n",
       "172  0.002621           that\n",
       "173  0.005635          thing\n",
       "174  0.004934          think\n",
       "175  0.003322         though\n",
       "176  0.003341        thought\n",
       "177  0.002948          three\n",
       "178  0.008936           time\n",
       "179  0.002334           told\n",
       "180  0.004350           took\n",
       "181  0.005599            tri\n",
       "182  0.005983            two\n",
       "183  0.006972            use\n",
       "184  0.004433           want\n",
       "185  0.003090          wasnt\n",
       "186  0.002545            way\n",
       "187  0.010844           week\n",
       "188  0.008522         weight\n",
       "189  0.004497    weight gain\n",
       "190  0.003975           well\n",
       "191  0.005376           went\n",
       "192  0.006288           work\n",
       "193  0.002880          worri\n",
       "194  0.003464           wors\n",
       "195  0.006200          worst\n",
       "196  0.002671          worth\n",
       "197  0.007953          would\n",
       "198  0.009650           year\n",
       "\n",
       "[199 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.DataFrame(data={'imp':clf_RF.feature_importances_,'vars':vectorizer.get_feature_names()})\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imp</th>\n",
       "      <th>vars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.015786</td>\n",
       "      <td>period</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.014690</td>\n",
       "      <td>month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.014341</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.012286</td>\n",
       "      <td>get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.011947</td>\n",
       "      <td>im</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.011823</td>\n",
       "      <td>ive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.011631</td>\n",
       "      <td>pill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.010844</td>\n",
       "      <td>week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.010839</td>\n",
       "      <td>day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.010339</td>\n",
       "      <td>take</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.009888</td>\n",
       "      <td>first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.009839</td>\n",
       "      <td>start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.009650</td>\n",
       "      <td>year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.009193</td>\n",
       "      <td>bleed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.009142</td>\n",
       "      <td>cramp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.008936</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.008899</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.008755</td>\n",
       "      <td>got</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.008531</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.008522</td>\n",
       "      <td>weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.008328</td>\n",
       "      <td>gain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.008293</td>\n",
       "      <td>birth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.008209</td>\n",
       "      <td>pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.008005</td>\n",
       "      <td>birth control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.007953</td>\n",
       "      <td>would</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.007759</td>\n",
       "      <td>depress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.007693</td>\n",
       "      <td>feel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007676</td>\n",
       "      <td>acn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.007461</td>\n",
       "      <td>insert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.007294</td>\n",
       "      <td>stop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.002853</td>\n",
       "      <td>next</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.002849</td>\n",
       "      <td>definit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.002832</td>\n",
       "      <td>bit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.002831</td>\n",
       "      <td>anyth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.002829</td>\n",
       "      <td>fine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.002803</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.002795</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.002784</td>\n",
       "      <td>old</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.002772</td>\n",
       "      <td>anoth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.002758</td>\n",
       "      <td>due</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.002755</td>\n",
       "      <td>reason</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>0.002738</td>\n",
       "      <td>symptom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.002737</td>\n",
       "      <td>half</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.002728</td>\n",
       "      <td>feel like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.002703</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002698</td>\n",
       "      <td>absolut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.002693</td>\n",
       "      <td>increas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.002671</td>\n",
       "      <td>worth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.002666</td>\n",
       "      <td>complet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.002660</td>\n",
       "      <td>hour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.002655</td>\n",
       "      <td>someth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0.002621</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.002604</td>\n",
       "      <td>end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002601</td>\n",
       "      <td>actual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.002599</td>\n",
       "      <td>around</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.002545</td>\n",
       "      <td>way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.002505</td>\n",
       "      <td>may</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.002454</td>\n",
       "      <td>everyon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.002384</td>\n",
       "      <td>negat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.002334</td>\n",
       "      <td>told</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          imp           vars\n",
       "130  0.015786         period\n",
       "113  0.014690          month\n",
       "108  0.014341           love\n",
       "70   0.012286            get\n",
       "90   0.011947             im\n",
       "97   0.011823            ive\n",
       "131  0.011631           pill\n",
       "187  0.010844           week\n",
       "38   0.010839            day\n",
       "168  0.010339           take\n",
       "66   0.009888          first\n",
       "161  0.009839          start\n",
       "198  0.009650           year\n",
       "20   0.009193          bleed\n",
       "35   0.009142          cramp\n",
       "178  0.008936           time\n",
       "103  0.008899           like\n",
       "74   0.008755            got\n",
       "32   0.008531        control\n",
       "188  0.008522         weight\n",
       "68   0.008328           gain\n",
       "17   0.008293          birth\n",
       "129  0.008209           pain\n",
       "18   0.008005  birth control\n",
       "197  0.007953          would\n",
       "41   0.007759        depress\n",
       "62   0.007693           feel\n",
       "1    0.007676            acn\n",
       "94   0.007461         insert\n",
       "163  0.007294           stop\n",
       "..        ...            ...\n",
       "122  0.002853           next\n",
       "40   0.002849        definit\n",
       "19   0.002832            bit\n",
       "9    0.002831          anyth\n",
       "65   0.002829           fine\n",
       "145  0.002803          right\n",
       "83   0.002795           high\n",
       "126  0.002784            old\n",
       "7    0.002772          anoth\n",
       "47   0.002758            due\n",
       "140  0.002755         reason\n",
       "167  0.002738        symptom\n",
       "77   0.002737           half\n",
       "63   0.002728      feel like\n",
       "146  0.002703           said\n",
       "0    0.002698        absolut\n",
       "93   0.002693        increas\n",
       "196  0.002671          worth\n",
       "30   0.002666        complet\n",
       "87   0.002660           hour\n",
       "159  0.002655         someth\n",
       "172  0.002621           that\n",
       "51   0.002604            end\n",
       "2    0.002601         actual\n",
       "10   0.002599         around\n",
       "186  0.002545            way\n",
       "111  0.002505            may\n",
       "55   0.002454        everyon\n",
       "119  0.002384          negat\n",
       "179  0.002334           told\n",
       "\n",
       "[199 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sort_values(by = 'imp' , ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 80% is the prediction by Random Forest by its default hyper-parameters. We now fine tune the model with best parameters so we can see the change, hopefully an increase, in the prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 'auto', 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "#gridsearch and hyperparameter tuning (on the TRAIN set) #THIS CELL IS 24K GOLD! \n",
    "param_grid = {'n_estimators': [100, 200, 300, 500], 'max_features': ['auto', None, 'log2']}\n",
    "clf_RF = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "clf_RF.fit(xtrain, ytrain)\n",
    "print(clf_RF.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 'auto', 'n_estimators': 100} 0.2268 0.0033\n",
      "{'max_features': 'auto', 'n_estimators': 200} 0.2258 0.003\n",
      "{'max_features': 'auto', 'n_estimators': 300} 0.2267 0.0026\n",
      "{'max_features': 'auto', 'n_estimators': 500} 0.2268 0.003\n",
      "{'max_features': None, 'n_estimators': 100} 0.2269 0.0036\n",
      "{'max_features': None, 'n_estimators': 200} 0.2259 0.0028\n",
      "{'max_features': None, 'n_estimators': 300} 0.2266 0.0027\n",
      "{'max_features': None, 'n_estimators': 500} 0.2268 0.0028\n",
      "{'max_features': 'log2', 'n_estimators': 100} 0.2268 0.0033\n",
      "{'max_features': 'log2', 'n_estimators': 200} 0.2258 0.003\n",
      "{'max_features': 'log2', 'n_estimators': 300} 0.2267 0.0026\n",
      "{'max_features': 'log2', 'n_estimators': 500} 0.2268 0.003\n"
     ]
    }
   ],
   "source": [
    "results_12gram = clf_RF.cv_results_\n",
    "for param, score_mean, score_sd in zip(results['params'], results['mean_test_score'], results['std_test_score']):\n",
    "    print(param, round(score_mean, 4), round(score_sd, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RF = RandomForestClassifier(n_estimators=500, max_features='auto', random_state=42)\n",
    "clf_RF.fit(xtrain, ytrain)\n",
    "clf_RF_predictions = clf_RF.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1331    0    2    0    0    0    0    6   16   74]\n",
      " [ 137  525    1    0    4    0    0    8   10   35]\n",
      " [ 118    4  493    0    4    0    0    0   12   31]\n",
      " [  75    0    3  384    2    0    0    4   29   45]\n",
      " [  70    0    6    0  557    2    0    8   28   58]\n",
      " [  47    0    2    0    0  328    0   13   17   39]\n",
      " [  43    0    0    0    2    0  483   11   43   47]\n",
      " [  52    2    4    0    0    0    0  790   77  127]\n",
      " [  59    2    2    0    0    0    0    7 1176  261]\n",
      " [  25    0    0    0    0    0    0    6   54 1808]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.68      0.93      0.79      1429\n",
      "         2.0       0.98      0.73      0.84       720\n",
      "         3.0       0.96      0.74      0.84       662\n",
      "         4.0       1.00      0.71      0.83       542\n",
      "         5.0       0.98      0.76      0.86       729\n",
      "         6.0       0.99      0.74      0.85       446\n",
      "         7.0       1.00      0.77      0.87       629\n",
      "         8.0       0.93      0.75      0.83      1052\n",
      "         9.0       0.80      0.78      0.79      1507\n",
      "        10.0       0.72      0.96      0.82      1893\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      9609\n",
      "   macro avg       0.90      0.79      0.83      9609\n",
      "weighted avg       0.85      0.82      0.82      9609\n",
      "\n",
      "0.8195441773337496\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(ytest,clf_RF_predictions))  \n",
    "print(classification_report(ytest,clf_RF_predictions))  \n",
    "print(accuracy_score(ytest, clf_RF_predictions)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surpisingly and gladly the performance of Random Forest on the (1,2) gram was tremendously improved. Here, we see an 82% accuracy on the prediction on the actual rating of between 1 to 10 of a review, which is very impressive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8195441773337496"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "clf_RF.score(xtest, ytest, sample_weight=None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking outside the corpus review rating\n",
    "\n",
    "In order to verify our machine learning performance, we choose a review from outside the entire data set so see the prediction of the review by our algorithm. \n",
    "The review we chose is: \"i loved this birth control. i used to have so much bloating with my previous pills and this one is honestly the best. my sex drive is better and I don't have side effects of wieght gain.\" \n",
    "This review of the birth control review is a 10-star. Let's see what our machine predicts through Random Forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_corpus_outside = normalize_corpus([\"i loved this birth control. i used to have so much bloating with my previous pills and this one is honestly the best. my sex drive is better and I don't have side effects of wieght gain.\"] ) \n",
    "# pre process, I do the count vectorizer, model.predict (on the verized form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vectorizer.transform(normalize_corpus_outside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction on the rating for this review is: [10.]\n"
     ]
    }
   ],
   "source": [
    "clf_RF.predict(x)\n",
    "print(\"The prediction on the rating for this review is: \" + str(clf_RF.predict(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells, we have normalized the corpus for other conditions, such as depression, pain and anxiety. We have followed all the steps that we did for the Birth Control and used Random Forest as our feature predicting model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Depression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_corpus_D = normalize_corpus(df2[df2['condition'] == 'Depression'].review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('depress', 9308),\n",
       " ('take', 8613),\n",
       " ('feel', 8433),\n",
       " ('mg', 7995),\n",
       " ('day', 6832),\n",
       " ('effect', 6674),\n",
       " ('year', 5806),\n",
       " ('week', 5664),\n",
       " ('side', 5424),\n",
       " ('work', 5353),\n",
       " ('start', 5261),\n",
       " ('month', 4714),\n",
       " ('anxieti', 4674),\n",
       " ('im', 4345),\n",
       " ('like', 4249),\n",
       " ('help', 4242),\n",
       " ('medic', 3905),\n",
       " ('life', 3884),\n",
       " ('time', 3825),\n",
       " ('tri', 3736),\n",
       " ('ive', 3729),\n",
       " ('get', 3585),\n",
       " ('first', 3486),\n",
       " ('felt', 3143),\n",
       " ('better', 3143),\n",
       " ('sleep', 2807),\n",
       " ('go', 2745),\n",
       " ('doctor', 2734),\n",
       " ('medicin', 2708),\n",
       " ('back', 2593),\n",
       " ('much', 2483),\n",
       " ('would', 2421),\n",
       " ('also', 2418),\n",
       " ('one', 2203),\n",
       " ('realli', 2192),\n",
       " ('drug', 2190),\n",
       " ('good', 2128),\n",
       " ('dose', 2090),\n",
       " ('still', 2037),\n",
       " ('weight', 1999)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n_words(normalize_corpus(df2[df2['condition'] == 'Depression'].review), n=40)  #None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed text vector is \n",
      " [[0 1 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Words for each feature:\n",
      "['abl', 'actual', 'ago', 'almost', 'also', 'alway', 'antidepress', 'anxieti', 'anyth', 'appetit', 'attack', 'away', 'back', 'bad', 'bed', 'best', 'better', 'cant', 'caus', 'celexa', 'chang', 'come', 'complet', 'could', 'couldnt', 'coupl', 'cri', 'cymbalta', 'day', 'depress', 'depress anxieti', 'didnt', 'differ', 'disord', 'doctor', 'dont', 'dose', 'drive', 'drug', 'due', 'eat', 'effect', 'effexor', 'energi', 'even', 'ever', 'everi', 'everyth', 'experi', 'experienc', 'extrem', 'far', 'feel', 'feel like', 'felt', 'final', 'first', 'first week', 'found', 'gain', 'gave', 'get', 'give', 'go', 'gone', 'good', 'got', 'great', 'happi', 'headach', 'help', 'high', 'hope', 'horribl', 'hour', 'howev', 'im', 'improv', 'increas', 'insomnia', 'issu', 'ive', 'know', 'last', 'less', 'lexapro', 'life', 'like', 'littl', 'long', 'lost', 'lot', 'love', 'made', 'major', 'make', 'mani', 'med', 'medic', 'medicin', 'mg', 'month', 'mood', 'morn', 'much', 'much better', 'nausea', 'need', 'never', 'night', 'normal', 'noth', 'notic', 'old', 'one', 'pain', 'panic', 'past', 'peopl', 'person', 'pill', 'posit', 'prescrib', 'pristiq', 'problem', 'prozac', 'put', 'quit', 'realli', 'recommend', 'say', 'see', 'seem', 'sever', 'sex', 'side', 'side effect', 'sinc', 'sleep', 'someth', 'start', 'start mg', 'start take', 'still', 'stop', 'suffer', 'suicid', 'switch', 'symptom', 'take', 'take mg', 'taken', 'thing', 'think', 'though', 'thought', 'time', 'tire', 'took', 'tri', 'two', 'use', 'want', 'way', 'week', 'weight', 'weight gain', 'well', 'wellbutrin', 'went', 'withdraw', 'within', 'without', 'wonder', 'work', 'wors', 'would', 'year', 'zoloft']\n"
     ]
    }
   ],
   "source": [
    "# So now, let's change the ngram to (1,2)\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2), min_df=0.05, max_df=1.0, max_features=500)\n",
    "\n",
    "# call `fit` to build the vocabulary\n",
    "vectorizer.fit(norm_corpus_D)\n",
    "\n",
    "# call `transform` to convert text to a bag of words\n",
    "x = vectorizer.transform(norm_corpus_D)\n",
    "\n",
    "# CountVectorizer uses a sparse array to save memory, but it's easier in this assignment to \n",
    "# convert back to a \"normal\" numpy array\n",
    "x = x.toarray()  # this is for visualization purposes \n",
    "\n",
    "print()\n",
    "print(\"Transformed text vector is \\n\", x)\n",
    "\n",
    "# `get_feature_names` tracks which word is associated with each column of the transformed x\n",
    "print\n",
    "print(\"Words for each feature:\")\n",
    "print(vectorizer.get_feature_names())  # visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df2[df2['condition'] == 'Depression'].rating  # df2['rating']\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y)  #(default=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nazanin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#regressor target variables rating is \n",
    "# Train the model with Random Foprest Classifier     \n",
    "clf_RF = RandomForestClassifier()  \n",
    "#n_jobs=2 number of cores the computer uses, n_estimators = 500 (number of trees)  startt 50 \n",
    "clf_RF.fit(xtrain, ytrain)  \n",
    "# predict and evaluate performance\n",
    "clf_RF_predictions = clf_RF.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00403743 0.00293845 0.00584924 0.0043932  0.00726728 0.0037476\n",
      " 0.0058153  0.0103228  0.0029234  0.00326871 0.00354678 0.00469821\n",
      " 0.00707556 0.00591681 0.00247209 0.00372176 0.00802306 0.00377743\n",
      " 0.00521608 0.0038862  0.00550516 0.0037061  0.00290006 0.00502601\n",
      " 0.00365341 0.00319737 0.00378681 0.00458883 0.01356203 0.01266173\n",
      " 0.00334667 0.0064951  0.00565808 0.00246259 0.00773281 0.00653911\n",
      " 0.00732382 0.00351998 0.00826284 0.00356587 0.00332787 0.00981057\n",
      " 0.00443697 0.00479073 0.00532518 0.00477784 0.0042282  0.00277781\n",
      " 0.00441894 0.00505709 0.00514092 0.00411013 0.01590123 0.00470816\n",
      " 0.00729207 0.00353696 0.00836153 0.00343788 0.00337364 0.00590171\n",
      " 0.00287309 0.00949885 0.0036486  0.00836927 0.00295644 0.00701837\n",
      " 0.00513867 0.00570425 0.00460049 0.00455335 0.01161411 0.00270649\n",
      " 0.00471333 0.00422732 0.00349162 0.00473942 0.01123247 0.0044876\n",
      " 0.00618887 0.00431485 0.00349838 0.00957047 0.00424598 0.00465496\n",
      " 0.00477218 0.00593357 0.01279759 0.00973789 0.00515272 0.00330052\n",
      " 0.0038269  0.00618115 0.00304625 0.00671715 0.00368286 0.00579367\n",
      " 0.00493567 0.00459325 0.01022303 0.00883564 0.01464575 0.01118501\n",
      " 0.00770223 0.00391937 0.00797008 0.00220359 0.00672517 0.00423183\n",
      " 0.00490667 0.00661052 0.00401483 0.00392742 0.0058261  0.00278386\n",
      " 0.00712593 0.00351582 0.00350089 0.00239025 0.00338978 0.00336403\n",
      " 0.00364342 0.00360401 0.00468346 0.00490865 0.00474236 0.00585928\n",
      " 0.00515706 0.00328674 0.00855721 0.00291139 0.00412589 0.00462414\n",
      " 0.00441213 0.00596341 0.00320703 0.00768    0.00770312 0.00563797\n",
      " 0.0083548  0.00260372 0.01102815 0.00265024 0.00366382 0.00665637\n",
      " 0.00738685 0.00375337 0.00464009 0.00410085 0.00421582 0.01314685\n",
      " 0.00349115 0.00464887 0.00535638 0.00555195 0.002702   0.00547841\n",
      " 0.0079788  0.00423162 0.0064452  0.00836335 0.0051602  0.00560522\n",
      " 0.00422258 0.00355406 0.01209714 0.00638299 0.00308897 0.00762949\n",
      " 0.00607641 0.00538387 0.00360773 0.00356686 0.00237709 0.00299622\n",
      " 0.01328513 0.00677653 0.006736   0.01198943 0.00568454]\n"
     ]
    }
   ],
   "source": [
    "print(clf_RF.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "print(clf_RF.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 'log2', 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "#gridsearch and hyperparameter tuning (on the TRAIN set) #THIS CELL IS 24K GOLD! \n",
    "param_grid = {'n_estimators': [100, 200, 300, 500], 'max_features': ['auto', None, 'log2']}\n",
    "clf_RF = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "clf_RF.fit(xtrain, ytrain)\n",
    "print(clf_RF.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RF = RandomForestClassifier(n_estimators=200, max_features='log2', random_state=42)\n",
    "clf_RF.fit(xtrain, ytrain)\n",
    "clf_RF_predictions = clf_RF.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[290   1   0   0   0   0   0   3  18  56]\n",
      " [ 17  81   0   0   0   0   1   4   4  27]\n",
      " [ 10   1  72   0   1   0   0   1   0  32]\n",
      " [  2   0   0  67   0   0   0   1  12  13]\n",
      " [  4   0   0   0  82   0   0   6   8  30]\n",
      " [  5   0   0   0   0  87   0   8   8  26]\n",
      " [  6   0   0   0   0   1 140  11  14  53]\n",
      " [  7   0   0   2   0   0   0 251  34 105]\n",
      " [  5   0   0   0   0   1   2   8 401 184]\n",
      " [ 11   0   0   0   1   1   0  16  31 778]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.81      0.79      0.80       368\n",
      "         2.0       0.98      0.60      0.75       134\n",
      "         3.0       1.00      0.62      0.76       117\n",
      "         4.0       0.97      0.71      0.82        95\n",
      "         5.0       0.98      0.63      0.77       130\n",
      "         6.0       0.97      0.65      0.78       134\n",
      "         7.0       0.98      0.62      0.76       225\n",
      "         8.0       0.81      0.63      0.71       399\n",
      "         9.0       0.76      0.67      0.71       601\n",
      "        10.0       0.60      0.93      0.73       838\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      3041\n",
      "   macro avg       0.88      0.68      0.76      3041\n",
      "weighted avg       0.79      0.74      0.74      3041\n",
      "\n",
      "0.7395593554751726\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(ytest,clf_RF_predictions))  \n",
    "print(classification_report(ytest,clf_RF_predictions))  \n",
    "print(accuracy_score(ytest, clf_RF_predictions)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the prediction is at 74%, which is 8% lower than the prediction for reviews on the birth contrl. We need to note that the review count for Depression is less than a third, 0.32, of the review counts for birth control so the machine learning techniques won't have sufficient data to train and hence predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12164"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[df2['condition'] == 'Depression'].review.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we attempt to tackle the prediction by using (2,2) gram but as we will see in the next few cells, the prediction rate is not as high as the (1,2) gram that we performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed text vector is \n",
      " [[0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Words for each feature:\n",
      "['depress anxieti', 'feel like', 'first week', 'much better', 'side effect', 'start mg', 'start take', 'take mg', 'weight gain']\n"
     ]
    }
   ],
   "source": [
    "# (2,2) gram\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2), min_df=0.05, max_df=1.0, max_features=500)\n",
    "# call `fit` to build the vocabulary\n",
    "vectorizer.fit(norm_corpus_D)   #fit is enough and then vectorizer.transform(normalized_outside_corpus)\n",
    "# call `transform` to convert text to a bag of words\n",
    "x = vectorizer.transform(norm_corpus_D)\n",
    "# CountVectorizer uses a sparse array to save memory, but it's easier in this assignment to \n",
    "# convert back to a \"normal\" numpy array\n",
    "x = x.toarray()  # this is for visualization purposes \n",
    "print()\n",
    "print(\"Transformed text vector is \\n\", x)\n",
    "# `get_feature_names` tracks which word is associated with each column of the transformed x\n",
    "print\n",
    "print(\"Words for each feature:\")\n",
    "print(vectorizer.get_feature_names())  # visualization \n",
    "y = df2[df2['condition'] == 'Depression'].rating  # df2['rating']\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y)  #(default=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nazanin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#regressor target variables rating is \n",
    "# Train the model with Random Foprest Classifier     \n",
    "clf_RF = RandomForestClassifier()  \n",
    "#n_jobs=2 number of cores the computer uses, n_estimators = 500 (number of trees)  startt 50 \n",
    "clf_RF.fit(xtrain, ytrain)  \n",
    "# predict and evaluate performance\n",
    "clf_RF_predictions = clf_RF.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09455699 0.12580011 0.09292928 0.07954719 0.15748121 0.09520075\n",
      " 0.13021056 0.11401512 0.1102588 ]\n"
     ]
    }
   ],
   "source": [
    "print(clf_RF.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "print(clf_RF.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': None, 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "#gridsearch and hyperparameter tuning (on the TRAIN set) #THIS CELL IS 24K GOLD! \n",
    "param_grid = {'n_estimators': [50, 100, 200, 300, 500], 'max_features': ['auto', None, 'log2']}\n",
    "clf_RF = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "clf_RF.fit(xtrain, ytrain)\n",
    "print(clf_RF.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RF = RandomForestClassifier(n_estimators=300, max_features=None, random_state=42)\n",
    "clf_RF.fit(xtrain, ytrain)\n",
    "clf_RF_predictions = clf_RF.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5   0   0   0   0   0   3   3  22 324]\n",
      " [  0   2   0   0   0   2   2   3   9 125]\n",
      " [  0   0   2   1   0   0   0   6  10  92]\n",
      " [  2   0   1   1   0   0   0   5   8  72]\n",
      " [  0   0   0   1   6   0   0   6  17 118]\n",
      " [  3   0   0   0   0   3   1   8  13 127]\n",
      " [  3   0   0   0   1   1   8  11  20 172]\n",
      " [  0   0   1   0   1   0   1  33  47 312]\n",
      " [  5   1   5   0   1   0   1  21  85 463]\n",
      " [  5   2   2   1   3   1   2  32  90 707]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.22      0.01      0.03       357\n",
      "         2.0       0.40      0.01      0.03       143\n",
      "         3.0       0.18      0.02      0.03       111\n",
      "         4.0       0.25      0.01      0.02        89\n",
      "         5.0       0.50      0.04      0.07       148\n",
      "         6.0       0.43      0.02      0.04       155\n",
      "         7.0       0.44      0.04      0.07       216\n",
      "         8.0       0.26      0.08      0.13       395\n",
      "         9.0       0.26      0.15      0.19       582\n",
      "        10.0       0.28      0.84      0.42       845\n",
      "\n",
      "   micro avg       0.28      0.28      0.28      3041\n",
      "   macro avg       0.32      0.12      0.10      3041\n",
      "weighted avg       0.30      0.28      0.19      3041\n",
      "\n",
      "0.2801709963827688\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(ytest,clf_RF_predictions))  \n",
    "print(classification_report(ytest,clf_RF_predictions))  \n",
    "print(accuracy_score(ytest, clf_RF_predictions)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time for evaluating the prediction on reviews on pain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_corpus_P = normalize_corpus(df2[df2['condition'] == 'Pain'].review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pain', 13862),\n",
       " ('take', 5484),\n",
       " ('work', 4136),\n",
       " ('day', 3902),\n",
       " ('mg', 3545),\n",
       " ('back', 2829),\n",
       " ('year', 2763),\n",
       " ('effect', 2486),\n",
       " ('help', 2341),\n",
       " ('time', 2181),\n",
       " ('medicin', 2140),\n",
       " ('doctor', 2102),\n",
       " ('medic', 1990),\n",
       " ('get', 1960),\n",
       " ('use', 1842),\n",
       " ('sever', 1804),\n",
       " ('side', 1801),\n",
       " ('hour', 1687),\n",
       " ('relief', 1557),\n",
       " ('tri', 1507),\n",
       " ('prescrib', 1507),\n",
       " ('feel', 1490),\n",
       " ('like', 1410),\n",
       " ('im', 1384),\n",
       " ('well', 1363),\n",
       " ('surgeri', 1316),\n",
       " ('ive', 1256),\n",
       " ('one', 1245),\n",
       " ('life', 1240),\n",
       " ('start', 1216),\n",
       " ('month', 1209),\n",
       " ('would', 1199),\n",
       " ('everi', 1171),\n",
       " ('also', 1147),\n",
       " ('patch', 1130),\n",
       " ('great', 1124),\n",
       " ('go', 1063),\n",
       " ('much', 1063),\n",
       " ('need', 1058),\n",
       " ('better', 1023)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n_words(normalize_corpus(df2[df2['condition'] == 'Pain'].review), n=40)  #None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed text vector is \n",
      " [[0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 1 0 ... 0 0 4]\n",
      " [0 0 0 ... 0 1 1]\n",
      " [0 0 0 ... 0 0 1]]\n",
      "Words for each feature:\n",
      "['abl', 'ago', 'almost', 'also', 'away', 'back', 'back pain', 'bad', 'best', 'better', 'caus', 'chronic', 'chronic pain', 'could', 'daili', 'day', 'didnt', 'differ', 'disc', 'doctor', 'dont', 'dose', 'drug', 'due', 'effect', 'er', 'even', 'everi', 'feel', 'felt', 'first', 'found', 'gave', 'get', 'give', 'given', 'go', 'good', 'got', 'great', 'headach', 'help', 'hour', 'im', 'ive', 'knee', 'last', 'leg', 'life', 'like', 'littl', 'long', 'lot', 'lower', 'made', 'make', 'manag', 'mani', 'med', 'medic', 'medicin', 'mg', 'migrain', 'month', 'much', 'neck', 'need', 'nerv', 'never', 'night', 'norco', 'noth', 'one', 'oxycontin', 'pain', 'pain relief', 'patch', 'percocet', 'pill', 'prescrib', 'problem', 'put', 'realli', 'relief', 'reliev', 'say', 'seem', 'sever', 'side', 'side effect', 'sinc', 'sleep', 'start', 'still', 'stop', 'suffer', 'surgeri', 'take', 'take mg', 'taken', 'thing', 'time', 'took', 'tramadol', 'tri', 'two', 'use', 'vicodin', 'week', 'well', 'went', 'without', 'wonder', 'work', 'work well', 'would', 'year']\n"
     ]
    }
   ],
   "source": [
    "# So now, let's change the ngram to (1,2)\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2), min_df=0.05, max_df=1.0, max_features=500)\n",
    "\n",
    "# call `fit` to build the vocabulary\n",
    "vectorizer.fit(norm_corpus_P)\n",
    "\n",
    "# call `transform` to convert text to a bag of words\n",
    "x = vectorizer.transform(norm_corpus_P)\n",
    "\n",
    "# CountVectorizer uses a sparse array to save memory, but it's easier in this assignment to \n",
    "# convert back to a \"normal\" numpy array\n",
    "x = x.toarray()  # this is for visualization purposes \n",
    "\n",
    "print()\n",
    "print(\"Transformed text vector is \\n\", x)\n",
    "\n",
    "# `get_feature_names` tracks which word is associated with each column of the transformed x\n",
    "print\n",
    "print(\"Words for each feature:\")\n",
    "print(vectorizer.get_feature_names())  # visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df2[df2['condition'] == 'Pain'].rating  # df2['rating']\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y)  #(default=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nazanin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#regressor target variables rating is \n",
    "# Train the model with Random Foprest Classifier     \n",
    "clf_RF = RandomForestClassifier()  \n",
    "#n_jobs=2 number of cores the computer uses, n_estimators = 500 (number of trees)  startt 50 \n",
    "clf_RF.fit(xtrain, ytrain)  \n",
    "# predict and evaluate performance\n",
    "clf_RF_predictions = clf_RF.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00507205 0.00462299 0.00565628 0.00893089 0.00652562 0.01489724\n",
      " 0.00703229 0.00728882 0.0065098  0.00914574 0.0070263  0.00687031\n",
      " 0.00419252 0.00536318 0.00547727 0.0167282  0.00662916 0.0054591\n",
      " 0.00761032 0.01218975 0.0097575  0.00781779 0.00843896 0.00676684\n",
      " 0.01551271 0.00752545 0.00791305 0.00759387 0.01159906 0.00437626\n",
      " 0.00819048 0.00639034 0.00616641 0.01327115 0.00702463 0.0083925\n",
      " 0.00916835 0.00951327 0.00583812 0.01149034 0.00604197 0.01701391\n",
      " 0.01266354 0.00929397 0.01067413 0.00582213 0.00806199 0.00540672\n",
      " 0.00840218 0.01186835 0.00806511 0.00644472 0.00619722 0.0063577\n",
      " 0.00661712 0.01046372 0.00500311 0.00594504 0.00537888 0.01479124\n",
      " 0.01421884 0.01584166 0.00620655 0.00908773 0.00986737 0.00495396\n",
      " 0.00861364 0.00592331 0.00578949 0.00685851 0.00593137 0.00736894\n",
      " 0.00947436 0.00465462 0.03419348 0.00570851 0.00894881 0.00645307\n",
      " 0.00622186 0.01121615 0.00573276 0.00625843 0.00821843 0.01172088\n",
      " 0.00724977 0.00439848 0.00673739 0.01312556 0.01065716 0.00829151\n",
      " 0.00525165 0.00882831 0.0104506  0.00840562 0.00658201 0.00528976\n",
      " 0.00981868 0.01931488 0.00394515 0.00610665 0.00553772 0.01243154\n",
      " 0.00864706 0.00637361 0.01028159 0.00601469 0.01383049 0.00763143\n",
      " 0.00855963 0.00882678 0.00516846 0.00690256 0.00443991 0.01875427\n",
      " 0.00408768 0.00911964 0.01298944]\n"
     ]
    }
   ],
   "source": [
    "print(clf_RF.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "print(clf_RF.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 'log2', 'n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "#gridsearch and hyperparameter tuning (on the TRAIN set) #THIS CELL IS 24K GOLD! \n",
    "param_grid = {'n_estimators': [50, 100, 200, 300, 500], 'max_features': ['auto', None, 'log2']}\n",
    "clf_RF = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "clf_RF.fit(xtrain, ytrain)\n",
    "print(clf_RF.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RF = RandomForestClassifier(n_estimators=300, max_features='log2', random_state=42)\n",
    "clf_RF.fit(xtrain, ytrain)\n",
    "clf_RF_predictions = clf_RF.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[115   2   0   0   0   1   4   2  12  58]\n",
      " [  0  33   0   0   0   0   2   5   5  14]\n",
      " [  3   0  25   0   0   0   0   2   6  14]\n",
      " [  2   0   0  31   0   0   0   1   4  11]\n",
      " [  2   0   0   1  41   0   0   0   3  19]\n",
      " [  2   0   0   0   0  28   2   1   3  25]\n",
      " [  5   0   0   0   0   0  63   6   8  42]\n",
      " [  5   0   0   2   0   0   0 178  31 103]\n",
      " [  4   0   2   0   1   2   1   9 262 162]\n",
      " [  4   2   0   1   3   2   2  13  30 640]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.81      0.59      0.68       194\n",
      "         2.0       0.89      0.56      0.69        59\n",
      "         3.0       0.93      0.50      0.65        50\n",
      "         4.0       0.89      0.63      0.74        49\n",
      "         5.0       0.91      0.62      0.74        66\n",
      "         6.0       0.85      0.46      0.60        61\n",
      "         7.0       0.85      0.51      0.64       124\n",
      "         8.0       0.82      0.56      0.66       319\n",
      "         9.0       0.72      0.59      0.65       443\n",
      "        10.0       0.59      0.92      0.72       697\n",
      "\n",
      "   micro avg       0.69      0.69      0.69      2062\n",
      "   macro avg       0.83      0.59      0.68      2062\n",
      "weighted avg       0.73      0.69      0.68      2062\n",
      "\n",
      "0.6867119301648884\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(ytest,clf_RF_predictions))  \n",
    "print(classification_report(ytest,clf_RF_predictions))  \n",
    "print(accuracy_score(ytest, clf_RF_predictions)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction on the reviews on pain is even lower and we believe this is due to the fact that the model does not have sufficient amount of data to be able to predict. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The following is a loop in which we can get predictions of all 4 conditions by utilizing Random Forest model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization complete: \n",
      "Vectorization complete: \n",
      "creating train test data: \n",
      "Training model: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nazanin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished: Birth Control\n",
      "[[1257   32   25   10   26    6    8   13    4   27]\n",
      " [  95  518   14    8    6    2    6   10    6   16]\n",
      " [  91   10  476    6   20    8    4    5   18   19]\n",
      " [  53    6    9  403    7    4    4   10    5   21]\n",
      " [  66    6   17   15  532    4    9   21   32   36]\n",
      " [  40    6   14    9   12  363    7   10   24   31]\n",
      " [  48    6    6    9    8    3  468   18   15   18]\n",
      " [  48   10    8    9   15    2   10  821   54   73]\n",
      " [  51   10   11    5    6   14   19   40 1165  170]\n",
      " [  75   13   17    8   18    4   17   43   95 1657]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.69      0.89      0.78      1408\n",
      "         2.0       0.84      0.76      0.80       681\n",
      "         3.0       0.80      0.72      0.76       657\n",
      "         4.0       0.84      0.77      0.80       522\n",
      "         5.0       0.82      0.72      0.77       738\n",
      "         6.0       0.89      0.70      0.78       516\n",
      "         7.0       0.85      0.78      0.81       599\n",
      "         8.0       0.83      0.78      0.80      1050\n",
      "         9.0       0.82      0.78      0.80      1491\n",
      "        10.0       0.80      0.85      0.83      1947\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      9609\n",
      "   macro avg       0.82      0.78      0.79      9609\n",
      "weighted avg       0.80      0.80      0.80      9609\n",
      "\n",
      "0.7971693204287646\n",
      "Normalization complete: \n",
      "Vectorization complete: \n",
      "creating train test data: \n",
      "Training model: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nazanin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished: Depression\n",
      "[[285   9   3   1   5   3   0  17  20  33]\n",
      " [ 18  86   2   2   0   4   5   1   9   7]\n",
      " [ 17   0  73   2   1   3   0   7  14  10]\n",
      " [ 10   1   0  52   3   0   3   2   5   5]\n",
      " [ 11   1   1   1  82   1   1   9  14   6]\n",
      " [ 10   0   0   0   0  83   0   8   7  18]\n",
      " [  7   0   4   4   0   2 144  10  29  21]\n",
      " [ 19   3   2   3   1   4  11 287  48  50]\n",
      " [ 33   5   6   3   0   3   9  33 408  81]\n",
      " [ 34   2   1   1   3   5  10  33  82 669]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.64      0.76      0.70       376\n",
      "         2.0       0.80      0.64      0.71       134\n",
      "         3.0       0.79      0.57      0.67       127\n",
      "         4.0       0.75      0.64      0.69        81\n",
      "         5.0       0.86      0.65      0.74       127\n",
      "         6.0       0.77      0.66      0.71       126\n",
      "         7.0       0.79      0.65      0.71       221\n",
      "         8.0       0.71      0.67      0.69       428\n",
      "         9.0       0.64      0.70      0.67       581\n",
      "        10.0       0.74      0.80      0.77       840\n",
      "\n",
      "   micro avg       0.71      0.71      0.71      3041\n",
      "   macro avg       0.75      0.67      0.71      3041\n",
      "weighted avg       0.72      0.71      0.71      3041\n",
      "\n",
      "0.713252219664584\n",
      "Normalization complete: \n",
      "Vectorization complete: \n",
      "creating train test data: \n",
      "Training model: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nazanin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished: Pain\n",
      "[[121   4   1   1   1   1   3  11  11  31]\n",
      " [  2  32   0   0   0   0   2   4  10  12]\n",
      " [  6   1  30   0   0   0   0   7   1   8]\n",
      " [  5   2   0  20   1   0   0   4   5   7]\n",
      " [  4   0   0   0  31   1   1   3   4  15]\n",
      " [  6   0   0   0   0  31   1   6   4  10]\n",
      " [  2   2   0   0   3   1  66  14  11  19]\n",
      " [ 12   3   1   0   1   3  10 187  39  49]\n",
      " [ 10   0   1   1   2   2   8  34 288  96]\n",
      " [ 30   2   2   0   1   1  13  43  88 556]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.61      0.65      0.63       185\n",
      "         2.0       0.70      0.52      0.59        62\n",
      "         3.0       0.86      0.57      0.68        53\n",
      "         4.0       0.91      0.45      0.61        44\n",
      "         5.0       0.78      0.53      0.63        59\n",
      "         6.0       0.78      0.53      0.63        58\n",
      "         7.0       0.63      0.56      0.59       118\n",
      "         8.0       0.60      0.61      0.61       305\n",
      "         9.0       0.62      0.65      0.64       442\n",
      "        10.0       0.69      0.76      0.72       736\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      2062\n",
      "   macro avg       0.72      0.58      0.63      2062\n",
      "weighted avg       0.67      0.66      0.66      2062\n",
      "\n",
      "0.6605237633365665\n",
      "Normalization complete: \n",
      "Vectorization complete: \n",
      "creating train test data: \n",
      "Training model: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nazanin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished: Anxiety\n",
      "[[126   4   0   0   1   4   0  10  12  30]\n",
      " [  6  31   1   0   2   0   0   0   4   6]\n",
      " [  6   1  24   0   0   0   0   6   9   9]\n",
      " [  4   2   0  19   2   2   0   4  13   9]\n",
      " [  3   0   0   0  26   0   1   8   7  17]\n",
      " [  3   0   0   0   0  25   0   2   3  10]\n",
      " [  5   0   0   0   0   0  66   5   8  21]\n",
      " [ 16   1   1   1   1   0   2 151  31  57]\n",
      " [ 16   1   2   0   2   1   2  24 231 107]\n",
      " [ 12   3   2   1   0   2   3  39  66 621]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.64      0.67      0.66       187\n",
      "         2.0       0.72      0.62      0.67        50\n",
      "         3.0       0.80      0.44      0.56        55\n",
      "         4.0       0.90      0.35      0.50        55\n",
      "         5.0       0.76      0.42      0.54        62\n",
      "         6.0       0.74      0.58      0.65        43\n",
      "         7.0       0.89      0.63      0.74       105\n",
      "         8.0       0.61      0.58      0.59       261\n",
      "         9.0       0.60      0.60      0.60       386\n",
      "        10.0       0.70      0.83      0.76       749\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      1953\n",
      "   macro avg       0.74      0.57      0.63      1953\n",
      "weighted avg       0.68      0.68      0.67      1953\n",
      "\n",
      "0.6758832565284179\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "accuracies = {}\n",
    "list_of_conditions = ['Birth Control', 'Depression', 'Pain', 'Anxiety']\n",
    "for condition in list_of_conditions:\n",
    "    norm_corpus = normalize_corpus(df2[df2['condition'] == condition].review)\n",
    "    print(\"Normalization complete: \")\n",
    "    vectorizer_loop = CountVectorizer(ngram_range=(1,2), min_df=0.05, max_df=1.0, max_features=500)\n",
    "    #x = vectorizer(normalize_corpus)\n",
    "    x = vectorizer_loop.fit_transform(norm_corpus)\n",
    "    print(\"Vectorization complete: \")\n",
    "    clf_RF_loop = RandomForestClassifier()\n",
    "    #x = vectorizer.transform(normalize_corpus)\n",
    "    y = df2[df2['condition']==condition]['rating']\n",
    "    print(\"creating train test data: \")\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x, y)  #(default=0.25) 4 objects are created and returend and we assign the 4 objects\n",
    "    # left side is an object \n",
    "    print(\"Training model: \")\n",
    "    model = clf_RF_loop.fit(xtrain,ytrain) #clf_RF.fit  train_model \n",
    "    models[condition]=model\n",
    "    clf_RF_loop_predictions = clf_RF_loop.predict(xtest)\n",
    "    accuracies[condition] =  confusion_matrix(ytest,clf_RF_loop_predictions)\n",
    "    print(\"finished: \" + condition)\n",
    "    print(confusion_matrix(ytest,clf_RF_loop_predictions))  \n",
    "    print(classification_report(ytest,clf_RF_loop_predictions))  \n",
    "    print(accuracy_score(ytest, clf_RF_loop_predictions)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extreme Gradient Boosted Model with tuned hyperparameters\n",
    "\n",
    "##### Why Use XGBoost?\n",
    "In this section, we attempt to use another predicting model, called XGBoost to see if it's worth it (time and memory-wise as well as predicting power-wise). \n",
    "The two reasons to use XGBoost are also the two goals of the project:\n",
    "\n",
    "+ Execution Speed.\n",
    "+ Model Performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\nazanin\\anaconda3\\lib\\site-packages (0.82)\n",
      "Requirement already satisfied: scipy in c:\\users\\nazanin\\anaconda3\\lib\\site-packages (from xgboost) (1.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\nazanin\\anaconda3\\lib\\site-packages (from xgboost) (1.15.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "mingw_path = r'C:\\mingw-w64\\mingw64\\bin'\n",
    "os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=42, silent=True,\n",
       "       subsample=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_xgb = xgb.XGBClassifier(seed=42)\n",
    "clf_xgb.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9. 10. 10. ...  1.  1. 10.]\n"
     ]
    }
   ],
   "source": [
    "clf_xgb_predictions = clf_xgb.predict(xtest)\n",
    "print(clf_xgb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08585033, 0.07299387, 0.07060201, ..., 0.11585876, 0.20801324,\n",
       "        0.08751033],\n",
       "       [0.0871696 , 0.05253462, 0.04884918, ..., 0.10958034, 0.19170658,\n",
       "        0.2445443 ],\n",
       "       [0.07005567, 0.03702347, 0.03938122, ..., 0.1725004 , 0.20429568,\n",
       "        0.23365495],\n",
       "       ...,\n",
       "       [0.38858914, 0.0945024 , 0.07692696, ..., 0.06481455, 0.08719738,\n",
       "        0.0855424 ],\n",
       "       [0.25477156, 0.16465004, 0.12848762, ..., 0.05594369, 0.04289082,\n",
       "        0.03272218],\n",
       "       [0.0521051 , 0.02895203, 0.01790675, ..., 0.09202512, 0.25231206,\n",
       "        0.46827883]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_xgb = clf_xgb.predict_proba(xtest)\n",
    "pred_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3392652721407014"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_xgb.score(xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result from Extreme Gradient Boosting has been a bit disappointing to say the least! However, we are glad that we gave that model a try to see its performance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and Closing Remarks \n",
    "\n",
    "\n",
    "We used the data set given by drugs.com and aimed to predict rating on reviews on reviews of various conditions in the data set. On the wrangling and visualization of the data, we listed the first 10 drug categories and also the top 10 conditions presented in the data set. We then plotted the histogram of the review counts and we found out that the review feature is bi-modal (heavy count on the higher review score of 9 and 10 as well as lower review score of 1). The histogram of the logarithm of useful counts of a review (number of people who found a review useful) turned out to be normally distributed. \n",
    "Throughout the visualization process, we plotted the bar charts of the drug names as well as that of the conditions of the drugs. Moreover, we plotted the first 10 most used Birth Control drugs since the condition of Birth Control was the highest condition name for the drug use, followed by Depression, Pain and Anxiety. On the statistical inference phase, we tested the original test and train sets on the condition of birth control, depression, pain and anxiety to see the proportions of these conditions in the two data sets they were split into.  We also tested the mean of the drug rating of the entire data set as well as train and test data sets and saw that there was no difference in any of the ratios in any of the sets, hence, we failed to reject the null hypothesis. Basically, it turned out that all the conditions were evenly distributed and the means of the entire data set as well as the train and test data sets are also the same. Hence, we failed to reject the null hypothesis based on the fact that the mean drug review rating for the total data set and test sets is the same. On the Machine Learning phase, we started looking at the text documents of the review column for the top four conditions. We processed and tokenized corpus of reviews to build features for predictive models and trained machine learning models to predict drug rating based on reviews and relevant metadata. The machine learning models we chose were Naive Bayes, Random Forest and Extreme Gradient Boosting and the best performance by far was by Random Forest with a predicting score as high as 82%. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
