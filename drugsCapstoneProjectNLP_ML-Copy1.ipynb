{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nazanin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import sklearn\n",
    "import scipy as sp\n",
    "import nltk\n",
    "import re \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.display.max_colwidth = 200\n",
    "%matplotlib inline\n",
    "#import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "#from matplotlib import pyplot\n",
    "import pylab\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from scipy.stats import norm \n",
    "from scipy import stats \n",
    "from statsmodels.stats.weightstats import ztest\n",
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Nazanin\\Downloads\\drugsCom_raw\\drugsComTest_raw.csv', sep=\"\\t\")\n",
    "df1 = pd.read_csv(r'C:\\Users\\Nazanin\\Downloads\\drugsCom_raw\\drugsComTrain_raw.csv', sep=\"\\t\")\n",
    "#display(df, df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'drugName',\n",
       " 'condition',\n",
       " 'review',\n",
       " 'rating',\n",
       " 'date',\n",
       " 'usefulCount']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.append(df1)\n",
    "list(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df2 = df2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0         drugName                     condition  \\\n",
      "0      163740      Mirtazapine                    Depression   \n",
      "1      206473       Mesalamine  Crohn's Disease, Maintenance   \n",
      "2      159672          Bactrim       Urinary Tract Infection   \n",
      "3       39293         Contrave                   Weight Loss   \n",
      "4       97768  Cyclafem 1 / 35                 Birth Control   \n",
      "\n",
      "                                                                                                                                                                                                    review  \\\n",
      "0  \"I&#039;ve tried a few antidepressants over the years (citalopram, fluoxetine, amitriptyline), but none of those helped with my depression, insomnia &amp; anxiety. My doctor suggested and changed ...   \n",
      "1  \"My son has Crohn&#039;s disease and has done very well on the Asacol.  He has no complaints and shows no side effects.  He has taken as many as nine tablets per day at one time.  I&#039;ve been v...   \n",
      "2                                                                                                                                                                            \"Quick reduction of symptoms\"   \n",
      "3  \"Contrave combines drugs that were used for alcohol, smoking, and opioid cessation. People lose weight on it because it also helps control over-eating. I have no doubt that most obesity is caused ...   \n",
      "4  \"I have been on this birth control for one cycle. After reading some of the reviews on this type and similar birth controls I was a bit apprehensive to start. Im giving this birth control a 9 out ...   \n",
      "\n",
      "   rating                date  usefulCount  \n",
      "0    10.0   February 28, 2012           22  \n",
      "1     8.0        May 17, 2009           17  \n",
      "2     9.0  September 29, 2017            3  \n",
      "3     9.0       March 5, 2017           35  \n",
      "4     9.0    October 22, 2015            4  \n"
     ]
    }
   ],
   "source": [
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Levonorgestrel                        4930\n",
       "Etonogestrel                          4421\n",
       "Ethinyl estradiol / norethindrone     3753\n",
       "Nexplanon                             2892\n",
       "Ethinyl estradiol / norgestimate      2790\n",
       "Ethinyl estradiol / levonorgestrel    2503\n",
       "Phentermine                           2085\n",
       "Sertraline                            1868\n",
       "Escitalopram                          1747\n",
       "Mirena                                1673\n",
       "Name: drugName, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.drugName.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Birth Control      38436\n",
       "Depression         12164\n",
       "Pain                8245\n",
       "Anxiety             7812\n",
       "Acne                7435\n",
       "Bipolar Disorde     5604\n",
       "Insomnia            4904\n",
       "Weight Loss         4857\n",
       "Obesity             4757\n",
       "ADHD                4509\n",
       "Name: condition, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.condition.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4     \"I have been on this birth control for one cycle. After reading some of the reviews on this type and similar birth controls I was a bit apprehensive to start. Im giving this birth control a 9 out ...\n",
      "6     \"I&#039;ve had the copper coil for about 3 months now. I was really excited at the thought of not taking hormones. I&#039;m good with pain however I nearly fainted with insertion, couldn&#039;t be...\n",
      "9     \"I was on this pill for almost two years. It does work as far as not getting pregnant however my experience at first was it didn&#039;t make a huge difference then 6 or 7 months into it my sex dri...\n",
      "30    \"I absolutely love this product and recommend to everyone. I know everyone&#039;s body is different, so it is not for everyone, but it is not the medicines fault. I have NO negative symptoms since...\n",
      "37    \"I was on this for 5 years (and birth control pills for about 12 years), and would have told you how fabulous it was.  &lt;List all the benefits everyone else has listed, here.&gt;  Then a friend ...\n",
      "Name: review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df2[df2['condition'] == 'Birth Control'].review.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48                                                                                                                                                                                              \"Works for me\"\n",
      "63     \"I was prescribed this for onset of anxiety and possible hormonal mood swings. I was not told by my doctor how it would make me feel or how hard coming off of it would be. I took one 37.5 mg capsu...\n",
      "83     \"I did not like this medication. For anxiety, I have also tried Hydroxyzine (Atarax). I guess this is just my personal body chemistry but I actually prefer Atarax to this unlike most people. This ...\n",
      "133    \"I&#039;m a 32 year old male and I&#039;ve been taking buspar for about 10 months. At first it did nothing but make my anxiety worse. I would wake up with to full blown panic attacks and have the ...\n",
      "208    \"Klonopin is a very effective medicine for people such as myself that suffer from debilitating panic disorder and/or PTSD.  This medicine saved me from becoming institutionalized after returning h...\n",
      "Name: review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df2[df2['condition'] == 'Anxiety'].review.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP \n",
    "+ On the Machine Learning phase, we start looking at the text documents of the review column. In order to do that, we need to do some text pre-processing. We process and tokenize corpus of reviews to build features for predictive models. Also, train machine learning models to predict drug rating based on reviews and relevant metadata.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text pre-processing\n",
    "\n",
    "+ Removing tags: Removing unnecessary HTML tags, etc. which do not add much value when analyzing text (The BeautifulSoup library does an excellent job in providing necessary functions for this.).\n",
    "\n",
    "+ Removing accented characters: Removing  accented characters\\letters in the text corpus and convert these characters and standardized into ASCII characters, an example is to convert é to e. \n",
    "\n",
    "+ Expanding contractions: Converting contraction to its expanded, original form often helps with text standardization, example of which would be, \"do not\" to \"don’t\" and \"I would\" to \"I’d\".\n",
    "\n",
    "+ Removing special characters: Removing special characters and symbols often add to the extra noise in unstructured text. More than often, simple regular expressions (regexes) can be used to achieve this.\n",
    "\n",
    "+ Stemming and lemmatization: The reverse process of inflection is called stemming, which is basically obtaining the base form of a word. Lemmatization is very similar to stemming, the difference being that the root word from lemmatization is always a lexicographically correct word but the root stem may not be so.\n",
    "\n",
    "+ Removing stopwords: Words with little or no significance in text corpus are known as stopwords, example of which are:  a, an, the. We can use a standard English language stopwords list from nltk and also add our own domain specific stopwords as needed.\n",
    "\n",
    "+ Other text cleaning to do would be tokenization, removing extra whitespaces, text lower casing as well as spelling corrections, grammatical error corrections, removing repeated characters to name a few. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#en_stopws = stopwords.words('english')  # this loads the default stopwords list for English\n",
    "#en_stopws.append('</span> users found this comment helpful.')  # add any words you don't like to the list </span> users found this comment helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove special characters, extra whitespaces, digits, stopwords and lower casing the text corpus\n",
    "#result = re.sub(pattern, repl, string, count=0, flags=0);\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)   ###plscheck this \n",
    "wpt = nltk.WordPunctTokenizer()  # $4.99 \n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words.append('</span> users found this comment helpful.') \n",
    "stop_words.append('&#039')\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A) \n",
    "    #re.I A means treats . as whatever it is. Performs case-insensitive matching. #issues with FLAGS!!!!!\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words] \n",
    "    filtered_tokens = [stemmer.stem(word) for word in filtered_tokens]  \n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "normalize_corpus = np.vectorize(normalize_document)\n",
    "# doc is the list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_corpus = normalize_corpus(df2[df2['condition'] == 'Birth Control'].review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['birth control one cycl read review type similar birth control bit apprehens start im give birth control long enough far love birth control side effect minim like im even birth control experienc mild headach nausea ive feel great got period cue third day inact pill idea come zero pms period light bare cramp unprotect sex first month obvious didnt get pregnant im pleas high recommend',\n",
       "       'ive copper coil month realli excit thought take hormon im good pain howev near faint insert couldnt beliv pain doctor say pain well month period last day im pain day random twang especi left side im consid whether want put intens pain heavi period id recommend somebodi doesnt alreadi heavi pain period right isnt',\n",
       "       'pill almost two year work far get pregnant howev experi first didnt make huge differ month sex drive went along dri moodi increas drastic would cri one second get angri husband anyth everyth skin gotten lot wors broke place never last week yaz',\n",
       "       ...,\n",
       "       'experi pain insert expect sinc ive never children lot bloat cramp breast tender first coupl week use pamprin heat pad belli help week insert came brown sludg last ten day first month studi miseri period stop breez occasion light cramp spot drawback person year old perimenopaus period make imposs track cycl overal delight',\n",
       "       'nexplanon sinc dec got first period end januari last month half march didnt bleed close three week start bleed march th bleed everi sinc gain lbs far sinc get birth control although weight gain isnt deal breaker bleed tri patient see bodi adjust implant three month far finger cross cycl go away awhil',\n",
       "       'would second month junel ive birth control year chang due spot increas mood swing previous birth control sinc switch shorter period day gain major weight increas appetit switch regular exercis routin still manag drop extra lbs'],\n",
       "      dtype='<U2960')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=None):\n",
    "    \"\"\"\n",
    "    List the top n words in a vocabulary according to occurrence in a text corpus.\n",
    "    \n",
    "    get_top_n_words([\"I love Python\", \"Python is a language programming\", \"Hello world\", \"I love the world\"]) -> \n",
    "    [('python', 2),\n",
    "     ('world', 2),\n",
    "     ('love', 2),\n",
    "     ('hello', 1),\n",
    "     ('is', 1),\n",
    "     ('programming', 1),\n",
    "     ('the', 1),\n",
    "     ('language', 1)]\n",
    "    \"\"\"\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('period', 37273),\n",
       " ('month', 36515),\n",
       " ('pill', 26390),\n",
       " ('get', 23071),\n",
       " ('ive', 21923),\n",
       " ('day', 21659),\n",
       " ('im', 20430),\n",
       " ('year', 19498),\n",
       " ('control', 18934),\n",
       " ('week', 18639),\n",
       " ('birth', 18338),\n",
       " ('take', 18146),\n",
       " ('first', 17184),\n",
       " ('cramp', 16278),\n",
       " ('gain', 15381),\n",
       " ('weight', 15367),\n",
       " ('start', 15324),\n",
       " ('effect', 14001),\n",
       " ('got', 13875),\n",
       " ('time', 13647),\n",
       " ('pain', 13167),\n",
       " ('like', 13144),\n",
       " ('side', 12075),\n",
       " ('bleed', 11932),\n",
       " ('would', 11297),\n",
       " ('insert', 11113),\n",
       " ('feel', 10858),\n",
       " ('sinc', 10519),\n",
       " ('mood', 10392),\n",
       " ('acn', 10089),\n",
       " ('never', 9849),\n",
       " ('spot', 9770),\n",
       " ('sex', 9433),\n",
       " ('bad', 9376),\n",
       " ('go', 9145),\n",
       " ('use', 9100),\n",
       " ('also', 9021),\n",
       " ('one', 8951),\n",
       " ('last', 8476),\n",
       " ('dont', 8321)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n_words(normalize_corpus(df2[df2['condition'] == 'Birth Control'].review), n=40)  #None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed text vector is \n",
      " [[5 0 1 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 1]\n",
      " [2 0 0 ... 0 0 0]]\n",
      "Words for each feature:\n",
      "['birth control', 'feel like', 'first month', 'gain weight', 'mood swing', 'sex drive', 'side effect', 'take pill', 'weight gain']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(2,2), min_df=0.05, max_df=1.0, max_features=500)\n",
    "\n",
    "# call `fit` to build the vocabulary\n",
    "vectorizer.fit(norm_corpus)\n",
    "\n",
    "# call `transform` to convert text to a bag of words\n",
    "x = vectorizer.transform(norm_corpus)\n",
    "\n",
    "# CountVectorizer uses a sparse array to save memory, but it's easier in this assignment to \n",
    "# convert back to a \"normal\" numpy array\n",
    "x = x.toarray()  # this is for visualization purposes \n",
    "\n",
    "print()\n",
    "print(\"Transformed text vector is \\n\", x)\n",
    "\n",
    "# `get_feature_names` tracks which word is associated with each column of the transformed x\n",
    "print\n",
    "print(\"Words for each feature:\")\n",
    "print(vectorizer.get_feature_names())  # visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38436, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df2[df2['condition'] == 'Birth Control'].rating  # df2['rating']\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y)  #(default=0.25)\n",
    "clf = MultinomialNB().fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = clf.score(xtrain, ytrain)\n",
    "test_accuracy = clf.score(xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MN Accuracy: 20.79%\n",
      "Accuracy on training data: 0.21\n",
      "Accuracy on test data:     0.21\n"
     ]
    }
   ],
   "source": [
    "print(\"MN Accuracy: %0.2f%%\" % (100 * clf.score(xtest, ytest)))\n",
    "print(\"Accuracy on training data: %0.2f\" % (training_accuracy))\n",
    "print(\"Accuracy on test data:     %0.2f\" % (test_accuracy))   #30% is precicely predicting the ratings for df2_min = 0.1\n",
    "# do a good and bad as a binary! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nazanin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    " #regressor target variables rating is \n",
    "# Train the model with Random Foprest Classifier     \n",
    "clf_RF = RandomForestClassifier()  \n",
    "#n_jobs=2 number of cores the computer uses, n_estimators = 500 (number of trees)  startt 50 \n",
    "clf_RF.fit(xtrain, ytrain)  \n",
    "# predict and evaluate performance\n",
    "clf_RF_predictions = clf_RF.predict(xtest)\n",
    "#meu.display_model_performance_metrics(true_labels=ytest, predicted_labels=clf_RF_predictions , classes= range(1,11))\n",
    "#hash maps a value to another value and it's more efficient in computation\n",
    "# normaization removing bias that could exists in a data set \n",
    "# correcting for the size of the corpus. one word review and the other is 200000 words \n",
    "# and contains the word good and we should \n",
    "# normaliztion in NLP is to use word frequecy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20584546 0.06917504 0.0826262  0.08909746 0.15301874 0.08619107\n",
      " 0.10124585 0.12122744 0.09157273]\n"
     ]
    }
   ],
   "source": [
    "print(clf_RF.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "print(clf_RF.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': None, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "#gridsearch and hyoerparameter tuning (on the TRAIN set) \n",
    "param_grid = {'n_estimators': [100, 200, 300, 500], 'max_features': ['auto', None, 'log2']}\n",
    "clf_RF = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "clf_RF.fit(xtrain, ytrain)\n",
    "print(clf_RF.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 'auto', 'n_estimators': 100} 0.2268 0.0033\n",
      "{'max_features': 'auto', 'n_estimators': 200} 0.2258 0.003\n",
      "{'max_features': 'auto', 'n_estimators': 300} 0.2267 0.0026\n",
      "{'max_features': 'auto', 'n_estimators': 500} 0.2268 0.003\n",
      "{'max_features': None, 'n_estimators': 100} 0.2269 0.0036\n",
      "{'max_features': None, 'n_estimators': 200} 0.2259 0.0028\n",
      "{'max_features': None, 'n_estimators': 300} 0.2266 0.0027\n",
      "{'max_features': None, 'n_estimators': 500} 0.2268 0.0028\n",
      "{'max_features': 'log2', 'n_estimators': 100} 0.2268 0.0033\n",
      "{'max_features': 'log2', 'n_estimators': 200} 0.2258 0.003\n",
      "{'max_features': 'log2', 'n_estimators': 300} 0.2267 0.0026\n",
      "{'max_features': 'log2', 'n_estimators': 500} 0.2268 0.003\n"
     ]
    }
   ],
   "source": [
    "results_22gram = clf_RF.cv_results_\n",
    "for param, score_mean, score_sd in zip(results['params'], results['mean_test_score'], results['std_test_score']):\n",
    "    print(param, round(score_mean, 4), round(score_sd, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best hyperparameter performance occurs at: {'max_features': None, 'n_estimators': 100} 0.2269 0.0036"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RF = RandomForestClassifier(n_estimators=100, max_features=None, random_state=42)\n",
    "clf_RF.fit(xtrain, ytrain)\n",
    "clf_RF_predictions = clf_RF.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 296   20   16    7    9    6   12   36   70  990]\n",
      " [ 145   23    7    6    8    4   11   30   45  464]\n",
      " [ 108   21   38    3    6    3    6   27   44  426]\n",
      " [ 100   11    3   14   11    6   12   29   30  297]\n",
      " [ 128   17    5    7   26    4    4   38   55  468]\n",
      " [  73   17    4    4    4   27    7   22   43  280]\n",
      " [  79    7    6    6    9    2   24   33   61  385]\n",
      " [ 144   17    6    7   14    6   17  123   63  651]\n",
      " [ 159   14   20    3   10    6   15   66  194  937]\n",
      " [ 169   29   10    3   10    7   17   75  125 1447]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.21      0.20      0.21      1462\n",
      "         2.0       0.13      0.03      0.05       743\n",
      "         3.0       0.33      0.06      0.10       682\n",
      "         4.0       0.23      0.03      0.05       513\n",
      "         5.0       0.24      0.03      0.06       752\n",
      "         6.0       0.38      0.06      0.10       481\n",
      "         7.0       0.19      0.04      0.07       612\n",
      "         8.0       0.26      0.12      0.16      1048\n",
      "         9.0       0.27      0.14      0.18      1424\n",
      "        10.0       0.23      0.76      0.35      1892\n",
      "\n",
      "   micro avg       0.23      0.23      0.23      9609\n",
      "   macro avg       0.25      0.15      0.13      9609\n",
      "weighted avg       0.24      0.23      0.17      9609\n",
      "\n",
      "0.23020085336663546\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(ytest,clf_RF_predictions))  \n",
    "print(classification_report(ytest,clf_RF_predictions))  \n",
    "print(accuracy_score(ytest, clf_RF_predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2560099906337808"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "clf_RF.score(x, y, sample_weight=None)  # on the entire data here so obviously it goes up by a few percentages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are performing ngram(1,2): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed text vector is \n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 1]\n",
      " ...\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 1]]\n",
      "Words for each feature:\n",
      "['absolut', 'acn', 'actual', 'ago', 'almost', 'also', 'alway', 'anoth', 'anxieti', 'anyth', 'around', 'away', 'back', 'bad', 'bc', 'best', 'better', 'birth', 'birth control', 'bit', 'bleed', 'bloat', 'bodi', 'break', 'breast', 'cant', 'caus', 'chang', 'clear', 'come', 'complet', 'constant', 'control', 'could', 'coupl', 'cramp', 'crazi', 'cri', 'day', 'decid', 'definit', 'depress', 'didnt', 'differ', 'doctor', 'dont', 'drive', 'due', 'eat', 'effect', 'emot', 'end', 'even', 'ever', 'everi', 'everyon', 'everyth', 'experi', 'experienc', 'extrem', 'face', 'far', 'feel', 'feel like', 'felt', 'fine', 'first', 'first month', 'gain', 'gain weight', 'get', 'give', 'go', 'good', 'got', 'gotten', 'great', 'half', 'happi', 'havent', 'headach', 'heavi', 'help', 'high', 'hope', 'hormon', 'horribl', 'hour', 'howev', 'hurt', 'im', 'implanon', 'implant', 'increas', 'insert', 'issu', 'iud', 'ive', 'know', 'last', 'lbs', 'life', 'light', 'like', 'littl', 'long', 'lost', 'lot', 'love', 'made', 'make', 'may', 'mirena', 'month', 'mood', 'mood swing', 'moodi', 'much', 'nausea', 'negat', 'never', 'nexplanon', 'next', 'normal', 'noth', 'notic', 'old', 'one', 'overal', 'pain', 'period', 'pill', 'pound', 'pregnanc', 'pregnant', 'pretti', 'problem', 'put', 'read', 'realli', 'reason', 'recommend', 'regular', 'remov', 'review', 'right', 'said', 'say', 'second', 'see', 'seem', 'sever', 'sex', 'sex drive', 'shot', 'side', 'side effect', 'sinc', 'skin', 'someth', 'spot', 'start', 'still', 'stop', 'sure', 'swing', 'switch', 'symptom', 'take', 'take pill', 'taken', 'terribl', 'that', 'thing', 'think', 'though', 'thought', 'three', 'time', 'told', 'took', 'tri', 'two', 'use', 'want', 'wasnt', 'way', 'week', 'weight', 'weight gain', 'well', 'went', 'work', 'worri', 'wors', 'worst', 'worth', 'would', 'year']\n"
     ]
    }
   ],
   "source": [
    "# So now, let's change the ngram to (1,2)\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2), min_df=0.05, max_df=1.0, max_features=500)\n",
    "\n",
    "# call `fit` to build the vocabulary\n",
    "vectorizer.fit(norm_corpus)\n",
    "\n",
    "# call `transform` to convert text to a bag of words\n",
    "x = vectorizer.transform(norm_corpus)\n",
    "\n",
    "# CountVectorizer uses a sparse array to save memory, but it's easier in this assignment to \n",
    "# convert back to a \"normal\" numpy array\n",
    "x = x.toarray()  # this is for visualization purposes \n",
    "\n",
    "print()\n",
    "print(\"Transformed text vector is \\n\", x)\n",
    "\n",
    "# `get_feature_names` tracks which word is associated with each column of the transformed x\n",
    "print\n",
    "print(\"Words for each feature:\")\n",
    "print(vectorizer.get_feature_names())  # visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38436, 199)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df2[df2['condition'] == 'Birth Control'].rating  # df2['rating']\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y)  #(default=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nazanin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    " #regressor target variables rating is \n",
    "# Train the model with Random Foprest Classifier     \n",
    "clf_RF = RandomForestClassifier()  \n",
    "#n_jobs=2 number of cores the computer uses, n_estimators = 500 (number of trees)  startt 50 \n",
    "clf_RF.fit(xtrain, ytrain)  \n",
    "# predict and evaluate performance\n",
    "clf_RF_predictions = clf_RF.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00261367 0.00780871 0.00252133 0.00427247 0.00475976 0.00751505\n",
      " 0.00422981 0.0029895  0.00387865 0.00303289 0.003107   0.00344073\n",
      " 0.00636469 0.00757787 0.00281829 0.00457    0.00369148 0.00854427\n",
      " 0.00801773 0.00296896 0.00851889 0.00289245 0.00459808 0.0031987\n",
      " 0.00480537 0.00408266 0.00405533 0.00489358 0.00357263 0.00328636\n",
      " 0.00311246 0.00455431 0.0087329  0.00345672 0.00323442 0.00871686\n",
      " 0.00280028 0.00386266 0.01123768 0.00280397 0.00291506 0.00748976\n",
      " 0.00576805 0.00413464 0.00574328 0.00648376 0.0046758  0.00281816\n",
      " 0.00275476 0.00655606 0.00385661 0.00273766 0.00479863 0.00475689\n",
      " 0.00606255 0.00273185 0.00266058 0.00506077 0.00473704 0.00443979\n",
      " 0.0030949  0.00570814 0.00755933 0.00275802 0.00418533 0.00295539\n",
      " 0.00919187 0.00404637 0.00882739 0.00310749 0.0119212  0.00311335\n",
      " 0.00710692 0.00543622 0.00855409 0.00340439 0.00508978 0.00253946\n",
      " 0.00307303 0.00487165 0.00479056 0.0044822  0.00452421 0.00231088\n",
      " 0.00449298 0.00381928 0.00534128 0.00272746 0.00457527 0.00290828\n",
      " 0.01154466 0.00331131 0.00373687 0.00340056 0.00828122 0.00348725\n",
      " 0.0029111  0.01160436 0.00345969 0.00612508 0.00370376 0.00371597\n",
      " 0.0048786  0.00817323 0.00509838 0.00349369 0.00280426 0.00477197\n",
      " 0.01380795 0.00530388 0.00431944 0.00245377 0.00340231 0.01513503\n",
      " 0.00605494 0.00550501 0.00351457 0.00512139 0.00354341 0.0023562\n",
      " 0.00750101 0.00365046 0.00282222 0.0050779  0.0038674  0.00574151\n",
      " 0.00264576 0.00715693 0.00395973 0.00804222 0.01571289 0.01247417\n",
      " 0.00456546 0.00451589 0.00553238 0.00345524 0.00461654 0.00498929\n",
      " 0.00303574 0.00641965 0.00273565 0.00576549 0.00306961 0.00554386\n",
      " 0.00364657 0.00255656 0.00274459 0.00388151 0.00337483 0.0034375\n",
      " 0.00289925 0.00446678 0.00646872 0.00393605 0.00316977 0.00600459\n",
      " 0.00517476 0.00784494 0.00395214 0.00293092 0.0077898  0.01000613\n",
      " 0.00499929 0.00667765 0.00299282 0.00533911 0.00557841 0.00282211\n",
      " 0.01022309 0.00391472 0.00305477 0.00377784 0.00305486 0.0053894\n",
      " 0.00473424 0.00343943 0.00339598 0.00342066 0.00900471 0.00258181\n",
      " 0.00415965 0.00589717 0.00594683 0.007501   0.00464417 0.00316699\n",
      " 0.00232613 0.01135039 0.00814229 0.00499299 0.00416511 0.00479723\n",
      " 0.00632275 0.00292573 0.00350533 0.005576   0.00296324 0.00730462\n",
      " 0.00887159]\n"
     ]
    }
   ],
   "source": [
    "print(clf_RF.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "print(clf_RF.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 'auto', 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "#gridsearch and hyoerparameter tuning (on the TRAIN set) #THIS CELL IS 24K GOLD! \n",
    "param_grid = {'n_estimators': [100, 200, 300, 500], 'max_features': ['auto', None, 'log2']}\n",
    "clf_RF = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "clf_RF.fit(xtrain, ytrain)\n",
    "print(clf_RF.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 'auto', 'n_estimators': 100} 0.2268 0.0033\n",
      "{'max_features': 'auto', 'n_estimators': 200} 0.2258 0.003\n",
      "{'max_features': 'auto', 'n_estimators': 300} 0.2267 0.0026\n",
      "{'max_features': 'auto', 'n_estimators': 500} 0.2268 0.003\n",
      "{'max_features': None, 'n_estimators': 100} 0.2269 0.0036\n",
      "{'max_features': None, 'n_estimators': 200} 0.2259 0.0028\n",
      "{'max_features': None, 'n_estimators': 300} 0.2266 0.0027\n",
      "{'max_features': None, 'n_estimators': 500} 0.2268 0.0028\n",
      "{'max_features': 'log2', 'n_estimators': 100} 0.2268 0.0033\n",
      "{'max_features': 'log2', 'n_estimators': 200} 0.2258 0.003\n",
      "{'max_features': 'log2', 'n_estimators': 300} 0.2267 0.0026\n",
      "{'max_features': 'log2', 'n_estimators': 500} 0.2268 0.003\n"
     ]
    }
   ],
   "source": [
    "results_12gram = clf_RF.cv_results_\n",
    "for param, score_mean, score_sd in zip(results['params'], results['mean_test_score'], results['std_test_score']):\n",
    "    print(param, round(score_mean, 4), round(score_sd, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RF = RandomForestClassifier(n_estimators=500, max_features='auto', random_state=42)\n",
    "clf_RF.fit(xtrain, ytrain)\n",
    "clf_RF_predictions = clf_RF.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1397    0    2    0    0    0    0    2   15   54]\n",
      " [ 140  512    3    0    0    0    0    2    3   37]\n",
      " [ 129    0  492    0    6    0    0    2   14   38]\n",
      " [  70    2    2  429    2    0    0    4   17   20]\n",
      " [ 100    0    4    0  532    0    0   15   31   48]\n",
      " [  40    0    0    2    2  360    0   16   33   41]\n",
      " [  44    2    0    0    6    0  448   17   52   65]\n",
      " [  34    0    2    0    0    0    0  794   72  140]\n",
      " [  35    0    0    0    2    0    2   17 1091  212]\n",
      " [  36    0    0    0    0    0    0    9   52 1859]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.69      0.95      0.80      1470\n",
      "         2.0       0.99      0.73      0.84       697\n",
      "         3.0       0.97      0.72      0.83       681\n",
      "         4.0       1.00      0.79      0.88       546\n",
      "         5.0       0.97      0.73      0.83       730\n",
      "         6.0       1.00      0.73      0.84       494\n",
      "         7.0       1.00      0.71      0.83       634\n",
      "         8.0       0.90      0.76      0.83      1042\n",
      "         9.0       0.79      0.80      0.80      1359\n",
      "        10.0       0.74      0.95      0.83      1956\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      9609\n",
      "   macro avg       0.90      0.79      0.83      9609\n",
      "weighted avg       0.85      0.82      0.82      9609\n",
      "\n",
      "0.823602872307212\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(ytest,clf_RF_predictions))  \n",
    "print(classification_report(ytest,clf_RF_predictions))  \n",
    "print(accuracy_score(ytest, clf_RF_predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "clf_RF.score(x, y, sample_weight=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imp</th>\n",
       "      <th>vars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002505</td>\n",
       "      <td>absolut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007727</td>\n",
       "      <td>acn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002709</td>\n",
       "      <td>actual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004506</td>\n",
       "      <td>ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005318</td>\n",
       "      <td>almost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.007333</td>\n",
       "      <td>also</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.004046</td>\n",
       "      <td>alway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.002746</td>\n",
       "      <td>anoth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.003640</td>\n",
       "      <td>anxieti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.002858</td>\n",
       "      <td>anyth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.003068</td>\n",
       "      <td>around</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.003485</td>\n",
       "      <td>away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.006176</td>\n",
       "      <td>back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.007039</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.003223</td>\n",
       "      <td>bc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.004238</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.003581</td>\n",
       "      <td>better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.008370</td>\n",
       "      <td>birth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.008033</td>\n",
       "      <td>birth control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.003159</td>\n",
       "      <td>bit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.008675</td>\n",
       "      <td>bleed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.003287</td>\n",
       "      <td>bloat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.004470</td>\n",
       "      <td>bodi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.002805</td>\n",
       "      <td>break</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.004591</td>\n",
       "      <td>breast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.003839</td>\n",
       "      <td>cant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.003991</td>\n",
       "      <td>caus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.004910</td>\n",
       "      <td>chang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.003724</td>\n",
       "      <td>clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.003229</td>\n",
       "      <td>come</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0.003912</td>\n",
       "      <td>take pill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0.002998</td>\n",
       "      <td>taken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.003872</td>\n",
       "      <td>terribl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0.002814</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.005382</td>\n",
       "      <td>thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0.004667</td>\n",
       "      <td>think</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.003331</td>\n",
       "      <td>though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.003395</td>\n",
       "      <td>thought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.003487</td>\n",
       "      <td>three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.008989</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.002543</td>\n",
       "      <td>told</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.004321</td>\n",
       "      <td>took</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.005699</td>\n",
       "      <td>tri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.005887</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0.007316</td>\n",
       "      <td>use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.004615</td>\n",
       "      <td>want</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.002999</td>\n",
       "      <td>wasnt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.002457</td>\n",
       "      <td>way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.010755</td>\n",
       "      <td>week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.008108</td>\n",
       "      <td>weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.004686</td>\n",
       "      <td>weight gain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.004183</td>\n",
       "      <td>well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.004949</td>\n",
       "      <td>went</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.006360</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.002964</td>\n",
       "      <td>worri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0.003352</td>\n",
       "      <td>wors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.005829</td>\n",
       "      <td>worst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.002693</td>\n",
       "      <td>worth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.007349</td>\n",
       "      <td>would</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.010417</td>\n",
       "      <td>year</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          imp           vars\n",
       "0    0.002505        absolut\n",
       "1    0.007727            acn\n",
       "2    0.002709         actual\n",
       "3    0.004506            ago\n",
       "4    0.005318         almost\n",
       "5    0.007333           also\n",
       "6    0.004046          alway\n",
       "7    0.002746          anoth\n",
       "8    0.003640        anxieti\n",
       "9    0.002858          anyth\n",
       "10   0.003068         around\n",
       "11   0.003485           away\n",
       "12   0.006176           back\n",
       "13   0.007039            bad\n",
       "14   0.003223             bc\n",
       "15   0.004238           best\n",
       "16   0.003581         better\n",
       "17   0.008370          birth\n",
       "18   0.008033  birth control\n",
       "19   0.003159            bit\n",
       "20   0.008675          bleed\n",
       "21   0.003287          bloat\n",
       "22   0.004470           bodi\n",
       "23   0.002805          break\n",
       "24   0.004591         breast\n",
       "25   0.003839           cant\n",
       "26   0.003991           caus\n",
       "27   0.004910          chang\n",
       "28   0.003724          clear\n",
       "29   0.003229           come\n",
       "..        ...            ...\n",
       "169  0.003912      take pill\n",
       "170  0.002998          taken\n",
       "171  0.003872        terribl\n",
       "172  0.002814           that\n",
       "173  0.005382          thing\n",
       "174  0.004667          think\n",
       "175  0.003331         though\n",
       "176  0.003395        thought\n",
       "177  0.003487          three\n",
       "178  0.008989           time\n",
       "179  0.002543           told\n",
       "180  0.004321           took\n",
       "181  0.005699            tri\n",
       "182  0.005887            two\n",
       "183  0.007316            use\n",
       "184  0.004615           want\n",
       "185  0.002999          wasnt\n",
       "186  0.002457            way\n",
       "187  0.010755           week\n",
       "188  0.008108         weight\n",
       "189  0.004686    weight gain\n",
       "190  0.004183           well\n",
       "191  0.004949           went\n",
       "192  0.006360           work\n",
       "193  0.002964          worri\n",
       "194  0.003352           wors\n",
       "195  0.005829          worst\n",
       "196  0.002693          worth\n",
       "197  0.007349          would\n",
       "198  0.010417           year\n",
       "\n",
       "[199 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.DataFrame(data={'imp':clf_RF.feature_importances_,'vars':vectorizer.get_feature_names()})\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imp</th>\n",
       "      <th>vars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.014883</td>\n",
       "      <td>month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.014685</td>\n",
       "      <td>period</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.013692</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.012514</td>\n",
       "      <td>pill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.011939</td>\n",
       "      <td>ive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.011831</td>\n",
       "      <td>get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.011769</td>\n",
       "      <td>im</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.011203</td>\n",
       "      <td>day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.010755</td>\n",
       "      <td>week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.010417</td>\n",
       "      <td>year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.010215</td>\n",
       "      <td>take</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.009806</td>\n",
       "      <td>first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.009552</td>\n",
       "      <td>start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.009522</td>\n",
       "      <td>cramp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.008989</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.008786</td>\n",
       "      <td>got</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.008675</td>\n",
       "      <td>bleed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.008637</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.008476</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.008405</td>\n",
       "      <td>gain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.008370</td>\n",
       "      <td>birth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.008283</td>\n",
       "      <td>pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.008108</td>\n",
       "      <td>weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.008033</td>\n",
       "      <td>birth control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007727</td>\n",
       "      <td>acn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.007555</td>\n",
       "      <td>sinc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.007516</td>\n",
       "      <td>feel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.007484</td>\n",
       "      <td>depress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.007450</td>\n",
       "      <td>insert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.007349</td>\n",
       "      <td>would</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.002930</td>\n",
       "      <td>complet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.002887</td>\n",
       "      <td>face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.002882</td>\n",
       "      <td>end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.002882</td>\n",
       "      <td>reason</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.002876</td>\n",
       "      <td>crazi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.002858</td>\n",
       "      <td>anyth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.002839</td>\n",
       "      <td>half</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.002839</td>\n",
       "      <td>hurt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0.002814</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.002805</td>\n",
       "      <td>break</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.002802</td>\n",
       "      <td>decid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.002785</td>\n",
       "      <td>everyth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.002782</td>\n",
       "      <td>read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.002755</td>\n",
       "      <td>said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.002749</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.002746</td>\n",
       "      <td>anoth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.002731</td>\n",
       "      <td>fine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.002710</td>\n",
       "      <td>everyon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002709</td>\n",
       "      <td>actual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.002703</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.002693</td>\n",
       "      <td>worth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.002692</td>\n",
       "      <td>hour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.002675</td>\n",
       "      <td>someth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.002553</td>\n",
       "      <td>old</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.002543</td>\n",
       "      <td>told</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.002537</td>\n",
       "      <td>may</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.002530</td>\n",
       "      <td>negat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002505</td>\n",
       "      <td>absolut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.002457</td>\n",
       "      <td>way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.002420</td>\n",
       "      <td>feel like</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          imp           vars\n",
       "113  0.014883          month\n",
       "130  0.014685         period\n",
       "108  0.013692           love\n",
       "131  0.012514           pill\n",
       "97   0.011939            ive\n",
       "70   0.011831            get\n",
       "90   0.011769             im\n",
       "38   0.011203            day\n",
       "187  0.010755           week\n",
       "198  0.010417           year\n",
       "168  0.010215           take\n",
       "66   0.009806          first\n",
       "161  0.009552          start\n",
       "35   0.009522          cramp\n",
       "178  0.008989           time\n",
       "74   0.008786            got\n",
       "20   0.008675          bleed\n",
       "32   0.008637        control\n",
       "103  0.008476           like\n",
       "68   0.008405           gain\n",
       "17   0.008370          birth\n",
       "129  0.008283           pain\n",
       "188  0.008108         weight\n",
       "18   0.008033  birth control\n",
       "1    0.007727            acn\n",
       "157  0.007555           sinc\n",
       "62   0.007516           feel\n",
       "41   0.007484        depress\n",
       "94   0.007450         insert\n",
       "197  0.007349          would\n",
       "..        ...            ...\n",
       "30   0.002930        complet\n",
       "60   0.002887           face\n",
       "51   0.002882            end\n",
       "140  0.002882         reason\n",
       "36   0.002876          crazi\n",
       "9    0.002858          anyth\n",
       "77   0.002839           half\n",
       "89   0.002839           hurt\n",
       "172  0.002814           that\n",
       "23   0.002805          break\n",
       "39   0.002802          decid\n",
       "56   0.002785        everyth\n",
       "138  0.002782           read\n",
       "146  0.002755           said\n",
       "83   0.002749           high\n",
       "7    0.002746          anoth\n",
       "65   0.002731           fine\n",
       "55   0.002710        everyon\n",
       "2    0.002709         actual\n",
       "145  0.002703          right\n",
       "196  0.002693          worth\n",
       "87   0.002692           hour\n",
       "159  0.002675         someth\n",
       "126  0.002553            old\n",
       "179  0.002543           told\n",
       "111  0.002537            may\n",
       "119  0.002530          negat\n",
       "0    0.002505        absolut\n",
       "186  0.002457            way\n",
       "63   0.002420      feel like\n",
       "\n",
       "[199 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sort_values(by = 'imp' , ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_corpus_outside = normalize_corpus(\"i loved this birth control\")  # pre process, I do the count vectorizer, model.predict (on the verized form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,2), min_df=0.05, max_df=1.0, max_features=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-df5ed124772d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalize_corpus_outside\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    996\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    997\u001b[0m         \"\"\"\n\u001b[1;32m--> 998\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    999\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1032\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1033\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    938\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_int_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: iteration over a 0-d array"
     ]
    }
   ],
   "source": [
    "vectorizer.fit(normalize_corpus_outside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "CountVectorizer - Vocabulary wasn't fitted.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-466123d34f80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm_corpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1081\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1083\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1084\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[1;34m\"\"\"Check if vocabulary is empty or missing (not fit-ed)\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"%(name)s - Vocabulary wasn't fitted.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vocabulary_'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 951\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: CountVectorizer - Vocabulary wasn't fitted."
     ]
    }
   ],
   "source": [
    "x = vectorizer.transform(norm_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Predict & Evaluate Extreme Gradient Boosted Model with tuned hyperparameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
